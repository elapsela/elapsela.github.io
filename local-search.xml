<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>卢宇 等｜生成式人工智能的教育应用与展望——以ChatGPT系统为例</title>
    <link href="/2023/06/10/6/"/>
    <url>/2023/06/10/6/</url>
    
    <content type="html"><![CDATA[<p><strong>【文章来源】</strong>卢宇, 余京蕾, 陈鹏鹤, &amp;李沐云. (2023). 生成式人工智能的教育应用与展望——以ChatGPT系统为例.<em>中国远程教育</em>(04).</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/Oa3Nicd3m3JMfHr6qNgBIHib4ZjV7K1jBqBs3wYSkdf6pOq0Fcz4P2Lwbgk4NlJEYCFFrXwUBk8mIzovHep9RT4A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1"></p><p><strong>【摘要】</strong>生成式人工智能（Generative Artificial Intelligence）旨在利用人工智能技术自动化生成文本、图像、视频、音频等多模态数据，受到教育领域的广泛关注。其中，ChatGPT系统因其良好的自然语言理解和生成能力，体现出较高的多领域应用潜力。本研究以ChatGPT作为主要对象，基于其四项核心能力，即启发性内容生成能力、对话情境理解能力、序列任务执行能力和程序语言解析能力，探讨在教师教学、学习过程、教育评价、学业辅导四个方面的潜在教育应用。在此基础上，在真实系统中进行了习题生成、自动解题、辅助批阅等教育应用的初步验证。最后，本文进一步探讨了以ChatGPT为代表的生成式人工智能在教育应用中所面临的局限和对教育的启示。</p><p><strong>【关键词】</strong>生成式人工智能；ChatGPT；大语言模型；人工智能教育应用</p><p>一、</p><p>引言</p><p>2017年7月由国务院印发的《新一代人工智能发展规划》中提出了我国对人工智能发展的战略规划，明确指出要抓住人工智能发展的重大历史机遇（国务院, 2017）。2022年11月，生成式人工智能系统ChatGPT正式发布（OpenAI, 2022），迅速成为教育领域关注和讨论的焦点。生成式人工智能（Generative Artificial Intelligence）指通过人工智能相关技术，自动化生成文本、图像、视频、音频等多类型内容。近年来，随着人工智能技术、算力水平与可获取数据量的提升，生成式人工智能技术依托语言、图像以及多模态大模型（Foundation Model），可以实现较好的内容生成效果（Bommasani et al., 2021），并在传媒、零售、法律、医疗、金融等领域逐步开始提供专业化与个性化内容生成服务。</p><p>针对生成式人工智能技术，国内外产业界与学术界都开展了较多的前期研发与投入。国内以百度为代表的AI企业致力于研发基于大模型的生成式人工智能系统，并实现快速落地。例如，ERNIE 3.0（Zhang et al., 2019）是基于知识增强的大语言模型，通过增强学习通识性知识，可进行具备知识可靠性的高质量文本创作；ERNIE-ViLG 2.0（Ho et al., 2020）作为多模态大模型，可通过输入文字描述，生成具备较好清晰度、可控性与创造性的图像，并基于扩散模型增强图文关键信息获取以及进行降噪过程优化。国外以OpenAI公司为典型代表，在跨模态内容生成、自然语言内容生成等领域，都做出了引领性贡献。例如，DALL·E 2（OpenAI, 2022）跨模态生成系统可依据用户文字描述，生成、扩展、修改和多样性迁移生成原创高清图片；GPT系列的人工智能系统可以生成文本类内容，逐步逼近实现类人的语言理解与交互能力，并于近期推出了基于大语言模型的多轮对话系统ChatGPT。</p><p>ChatGPT系统一经问世，便引发了产业界和学术界的广泛关注，用户规模迅速超过一亿，并在互联网领域迅速实现落地应用。微软“必应”搜索已开始借助ChatGPT，为用户提供结构化整合搜索结果、建议及聊天等功能，显著提升了搜索效率、改善了用户体验。在智能性方面，斯坦福学者依据心智理论测试发现GPT-3.5（ChatGPT的同源模型）可解决100%的意外迁移任务以及85%的意外内容任务，相当于9岁儿童的水平（Kosinski, 2023）；在专业考试方面，ChatGPT及其同源产品可基本通过谷歌L3级软件工程师水平测试、美国执业医师资格考试、美国司法考试中的证据和侵权行为测试、美国宾夕法尼亚大学沃顿商学院MBA运营管理课程考试等。</p><p>ChatGPT在文本类内容生成、上下文情境理解等方面所表现出的卓越性能，对教育领域也产生了巨大影响和深刻的启示意义，并可能促进和催化从教育理念到教育实践的深层次变革。长期受技术条件制约的启发式教学与个性化反馈等潜在智能教育应用也开始成为可能。本研究将以ChatGPT为主要研究对象，基于其技术维度的核心能力分析其在教育中的潜在应用，在对部分应用进行初步验证的基础上，探讨其局限性及对教育的启示。</p><p>二、</p><p>ChatGPT概述</p><p><strong>（一）历史演变</strong></p><p>人工智能领域的研究目标是通过模拟人类智能，使机器能够像人类一样思考和行动。科学家并从模仿人类语言交互的角度提出了著名的“图灵测试”。人工智能发展初期，主要关注知识形式化表征与符号化推理，但一直难以处理复杂多变且具有较强歧义性的人类自然语言。进入21世纪，研究人员开始尝试构建基于统计推断和机器学习技术的自然语言处理模型。虽然这些模型可以提升典型自然语言处理任务的性能，但是仍然难以深入分析和准确生成人类的自然语言。随着深度学习技术的发展，对人类自然语言进行高维分布式表征和隐含特征提取逐渐成为可能，人工智能技术在多项自然语言处理任务上的表现也有了极大提升。2017年谷歌公司提出Transformer（Vaswani et al., 2017）模型，促使自然语言处理模型的参数量得到大幅扩展。在此基础上，研究人员提出了预训练语言模型的概念，即基于大规模语料库并利用自监督学习技术训练语言模型，以提升机器对自然语言的理解能力，并由此开启了自然语言处理领域的大模型时代。</p><p>2018年6月，美国OpenAI公司提出了基于Transformer的预训练语言模型（Generative Pre-trained Transformer）GPT-1。GPT-1（Radford et al., 2018）基于自回归理念，采用12个Transformer解码器，构建从左向右单向预测的语言模型，参数量达1.17亿。GPT-1的构建首先基于大型语料库进行无监督的预训练，然后通过有监督的微调技术为下游自然语言处理任务提供解决方案。同年10月，谷歌推出了基于自编码理念的BERT（Bidirectional Encoder Representations from Transformers）模型，基于多层的Transformer编码器，采用从左右双向进行填空学习的方式开展训练（Devlin et al., 2019）。2019年2月，OpenAI发布了GPT-2模型（Radford et al., 2019），其核心理念与GPT-1相似，但采用了更多的Transformer解码器和更大的语料库进行训练，参数量达15亿。GPT-2在多项自然语言处理任务上均有较为出色的表现。同年谷歌进一步提出了T5（Raffel et al., 2019）模型，并在机器翻译与知识问答等任务上表现出更好的性能。2020年5月，OpenAI推出GPT-3（Brown et al., 2020），其参数量相较GPT-2提升了两个数量级，达到1,750亿。参数量的提升使GPT-3在对话生成、文本摘要、机器翻译等任务上展现出了卓越的性能。2022年初，OpenAI在GPT-3的基础上推出InstructGPT（Ouyang et al., 2022），并于同年11月推出其同源模型ChatGPT，在文本生成以及自然语言交互等任务上实现了较为惊人的进步。</p><p><strong>（二）相关技术</strong></p><p>为实现高质量的生成内容，以ChatGPT为代表的GPT系列系统，主要涉及了五项关键技术和架构。</p><p><strong>1. Transformer模型</strong></p><p>该模型是ChatGPT等系统的基本组成单元，本质上是一种基于自注意力机制的深度神经网络，主要包含编码器和解码器两部分。编码器主要包括一个自注意力子层和一个全连接前馈神经网络子层：前者计算输入序列中不同位置之间的依赖关系结构并进行特征表示，后者则对新生成的特征表示进行处理，生成最终的表征向量。解码器的基本结构与编码器类似，但针对编码器的输出增加了新的多头注意力层，并加入了掩码设定，以防止解码过程中后继位置信息泄漏。Transformer模型能够高效捕捉序列数据中不同位置之间的依赖关系，并处理任意长度的自然语言序列数据。</p><p><strong>2. 基于Transformer的基本架构</strong></p><p>GPT系列系统基于Transformer模型构成其基本系统架构，由于ChatGPT系统的具体技术结构信息目前并没有被完整披露，我们以其前身GPT-3为例进行介绍。如图1所示，GPT-3主要是由96层的Transformer 解码器组成，其中每层的解码器包含掩码多头注意力机制子层和全连接前馈神经网络子层，单词的嵌入维度和上下文窗口长度均进行了扩展，且采用稀疏注意力模式提升运行效率。模型训练的过程基于自回归思想，即给定上文内容预测下文单词或给定下文内容预测上文单词。此外，针对不同自然语言处理任务，GPT-3转换不同格式的文本语料进行模型训练。例如，针对机器翻译任务，训练数据会转换成类似“翻译成英文：他来自中国。He is from China.”格式的文本。这些技术与思想直接帮助GPT系列系统逐步具备了优秀的文本生成能力。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/Oa3Nicd3m3JMfHr6qNgBIHib4ZjV7K1jBqeLBInQMaib8UVlWNMrbfhzdNsZicLQQPlVhNxvc2NFicsGS82qcYHYVKw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图1 GPT-3模型的基本架构"></p><p><strong>3. 基于人类反馈的强化学习（Reinforcement Learning from Human Feedback, RLHF）技术</strong></p><p>该技术是ChatGPT内容生成能力提升的关键（Christinao et al., 2017）。RLHF技术包含三个步骤：第一步是监督式微调，其核心理念是利用符合人类预期的少量标注数据对预训练模型参数进行调整，初步优化文本生成模型；第二步是构建奖励模型，核心目标是通过对监督式微调生成的多个结果进行人工排序标记，训练奖励函数模型，用于强化学习模型输出结果的自动化评价；第三步是利用近端策略优化（Proximal Policy Optimization, PPO）算法，结合奖励模型对文本生成模型的结果进行自动评估，并采用强化学习对文本生成模型进行优化，使其最终具备生成符合人类预期的文本的能力（Schulman et al., 2017）。</p><p><strong>4. 指示微调（Instruction Tuning）技术</strong></p><p>该技术可以辅助ChatGPT等系统生成高质量的文本（Wei et al., 2021）。指示微调是一项语言模型训练技术，通过将预设的指令描述与生成文本相结合，构建训练数据，从而微调文本生成模型的参数。其核心思想是将自然语言处理相关任务转化为基于指令描述的文本生成问题。基于指令描述，可以促使模型理解指令任务，从而生成预期文本。例如，用户输入“判断这句话的对错：三角形的内角和为360度”，其中“判断这句话的对错”是指令描述，指令任务是对“三角形的内角和为360度”进行正误判断，从而生成预期的答案文本“该句话错误”。</p><p><strong>5. 思维链（Chain of Thought）技术</strong></p><p>该技术通过一系列前后关联的指令，可以辅助ChatGPT等系统完成复杂推理任务（Wei et al., 2022）。语言模型虽然在对人类自然语言表征和建模上取得了显著进展，但在复杂逻辑问题推理上，仍较难达到满意的效果。思维链技术可以针对性地设计模型的指令输入，促使模型将单步骤推理任务拆解为包含多个中间步骤的任务。其中，每一个中间步骤由一个相对简单的指令输入作为引导，其结果代表了多步骤任务的逻辑分析过程。思维链技术可以引导文本类内容生成，辅助模型生成和解决复杂逻辑推理任务。</p><p>同时，为解决文本生成过程中产生与用户预期不符的行为问题，例如捏造事实、生成有偏见或有害文本、不遵循用户指示等，ChatGPT等系统的技术研发遵循3H基本原则，即帮助性（Helpful）——模型应帮助用户解决问题、真实性（Honest）——模型不能捏造信息或误导用户、无害性（Harmless）——模型不能对人或环境造成身体、心理或社会性的伤害（Askell et al., 2021；Ouyang et al., 2022）。</p><p><strong>（三）核心能力</strong></p><p>上述相关技术与基本原则的科学合理使用，促使ChatGPT等系统在自然语言理解与内容生成方面，显示出以下四项较为突出的核心能力。</p><p><strong>1. 启发性内容生成能力</strong></p><p>ChatGPT等系统能够基于给定的主题或在多轮对话过程中识别的上下文信息，生成有启发性和创意性的文本，包括诗词、故事、评论等。这些文本不仅可以成为用户的创作素材，也可以在思维广度上为用户带来启发。</p><p><strong>2. 对话情境理解能力</strong></p><p>ChatGPT等系统能够基于多轮对话中的上下文信息，进行语义理解和推理，捕捉用户意图与对话情境，生成符合逻辑的连贯性回复，为用户带来良好的交互体验。</p><p><strong>3. 序列任务执行能力</strong></p><p>ChatGPT等系统能够基于用户的序列指令描述，理解指令之间的关联关系，逐步对任务进行推进，完成基于组合指令的复杂任务，从而较好地执行用户给出的多步骤序列任务。</p><p><strong>4. 程序语言解析能力</strong></p><p>ChatGPT等系统能够根据多种编程语言的语法规则、数据结构、算法构建与编程规范，对代码程序进行结构与算法分析，并根据用户任务需求自动生成符合任务要求的代码程序或错因解析。</p><p>上述四项核心能力体现了当前生成式人工智能领域的重要突破和价值，也为其在教育领域的应用提供了诸多可能性。</p><p>三、</p><p>潜在教育应用</p><p>基于当前ChatGPT系统所具备的启发性内容生成、对话情境理解、序列任务执行、程序语言解析四项核心能力，我们从教、学、评、辅四个典型教育环节，梳理了不同核心能力可以支持的潜在教育应用，如图2所示。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/Oa3Nicd3m3JMfHr6qNgBIHib4ZjV7K1jBqULyU4gX9hT0QLUDlu9j4YNwJ7iaSc0uYXY4qicg1XuFQeQOnJrFzMicBg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图2 ChatGPT的潜在教育应用"></p><p><strong>（一）教师教学</strong></p><p>ChatGPT等系统可以为教师教学提供多种形态的帮助和服务。基于启发性内容生成能力，ChatGPT等系统可以依据教师的教学目标生成创作型教学素材，辅助教师设计有创新性的教学活动；基于对话情境理解能力，ChatGPT等系统可以在课堂教学中充当助教角色，根据当前教学活动情境，为教师提供教学过程的交互式支持；基于序列任务执行能力，ChatGPT等系统可以依据教学场景与个体教师的教学需求，生成个性化教学方案；基于程序语言解析能力，ChatGPT等系统可以为教师提供编程课程案例，支持典型问题及其变形的示例性代码生成与说明。</p><p>具体而言，在辅助教师生成个性化教学方案的过程中，ChatGPT等系统可以依据教师的教学需求，分步骤生成多种适切的教学设计，为教师在备课过程中提供思路启发与多种备选方案，提升教师的备课效率与授课质量。例如，在中学语文课程《荷塘月色》的教案设计过程中，教师可以首先要求系统制作一份基础方案，包括教学主题、教学目标、教学内容、教学步骤、教学方法、教学评价、教学资源等模块。在此基础上，教师可以继续要求系统增加互动环节，并自动生成朱自清生平小测验、荷塘故事续写等趣味教学活动，供自己参考和选择性补充到教案中。教师也可以要求系统增加课外拓展素材，系统则会从作者的其他代表作品、书信与日记、故乡文化等方面提供参考教学资源。</p><p><strong>（二）学习过程</strong></p><p>ChatGPT等系统也可以为学习过程提供良好的支持和服务。基于启发性内容生成能力，ChatGPT等系统可以自动生成范文段落示例或启发性思路提示，为学生提供创意写作素材，以人机协同共创方式辅助学生写作；基于对话情境理解能力，ChatGPT等系统可以识别学生学习情境，结合学科专业知识，为学生提供基于情境的学科知识问答；基于序列任务执行能力，ChatGPT等系统可以依据学生的学习需求与知识掌握情况，为学生提供动态教学支架与反馈，并优化其学习路径；基于程序语言解析能力，ChatGPT等系统可以结合学生实际需求与编程任务，为学生推荐相关代码片段与运行解析，帮助其高效理解程序的设计思路与编写方式。</p><p>具体而言，在支持学生学科知识问答的过程中，ChatGPT系统可以基于多轮对话，为学生提供从现象分析、知识点讲解、应用影响等多层次服务。例如，系统可以为学生解答“苹果会落地”的物理学原理是地球引力作用，并进一步为学生讲解牛顿运动定律的知识点。如果学生继续提问“这些原理和定律的用途”，系统可以准确理解其问题指向，并从日常生活到航空航天等角度进行解答并做合理扩展。</p><p><strong>（三）教育评价</strong></p><p>ChatGPT等系统还可以针对性完成教育评价的多项任务。基于启发性内容生成能力，ChatGPT等系统可以为学生作品和答案进行客观点评，引导学生发掘作品优点并提供改进思路；基于对话情境理解能力，ChatGPT等系统可以对学生的语言表达能力进行评测，通过分析学生对话过程中的词汇、语法、句子结构，以及观点表达与事件描述方式，给出针对性的反馈建议；基于序列任务执行能力，ChatGPT等系统可以依据测试科目、考察目标、题目类型等组卷需求，自动生成多种备选测试题目，支持教师智能组卷；基于程序语言解析能力，ChatGPT等系统可以进行高效代码反馈与评价，纠正代码错误并提出优化建议。</p><p>具体而言，在为学生提供程序代码评价中，ChatGPT等系统可以通过自动识别程序语言、数据结构、函数类型与代码结构，整体评价典型算法程序的编写正误，并提供关于代码规范性、复杂度等多个维度的细颗粒度反馈与评价。例如，学生输入指令“请对下面这段代码进行评价反馈”并提供代码，系统可以指出该典型算法是否编写正确，并提供针对性建议与改进代码示例。此外，系统还可以从函数命名方式、函数功能注释、参数合法性检验、返回值设计、变量命名等方面提供改进提示。 </p><p><strong>（四）学业辅导</strong></p><p>ChatGPT等系统还可以尝试完成较为复杂和专业的学业辅导任务。基于启发性内容生成能力，ChatGPT等系统可以生成针对性资源和素材，引导学生从不同角度对知识点进行理解，辅助提升学生的知识探究与创新能力；基于对话情境理解能力，ChatGPT等系统可以依据历史对话信息理解学生的实际辅导需求，结合当前学习内容，为学生提供个性化学习支持；基于序列任务执行能力，ChatGPT等系统可以针对学生的疑难问题进行分步骤解析，帮助学生理解问题求解的要点与难点；基于程序语言解析能力，ChatGPT等系统可以生成多维度代码解释，包括参数设置、算法思路、逻辑关系等，帮助学生理解程序内涵与功能，辅助提升学生编程能力。</p><p>具体而言，在为学生生成知识探究与创新素材的过程中，ChatGPT等系统除可以生成以事实为依托的素材外，还可以生成基于假设的启发性素材，引导学生从不同维度解构知识，培养学生的思辨能力与探究意识。例如，当学生提问“如果荆轲刺秦王成功了，将会发生什么”，系统可以基于这种假设，为学生分析秦国是否还能统一六国以及中国历史的多种可能发展走向，启发学生对历史问题进行深入思考。</p><p>四、</p><p>教育应用初步验证</p><p>本研究从教师教学、学习过程与教育评价三个维度，分别选取题目生成、自动解题与辅助批阅三个具体教育应用，对系统进行初步验证。所选用的系统是2023年2月最新版本的ChatGPT。</p><p><strong>（一）题目生成</strong></p><p>如表1所示，当用户给出数学应用题生成的指令后，系统可以自动生成符合指令要求且具备合理情境信息的题目及其答案，即通过小明购买水果的情境设置考察乘法与加法的运算。在此基础上，如果给出更改情境的指令，系统会进一步生成小明购买文具的情境并考查相同的数学知识点。最后，如果继续用中文给出“请再出一道类似的英文习题”的指令，系统可以生成描述清晰且考察知识点相同的英文题目，而其具体情境可以有所不同。 </p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/Oa3Nicd3m3JMfHr6qNgBIHib4ZjV7K1jBqWGE6D6oIGwjp91Q7jibTL9VPF1FnhCHrWyx3nAg2GGmYDRBUy9Cx9vg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="表1 习题生成示例"></p><p>经过多轮测试，系统可以持续生成质量和难度均适中的多学科、多情境习题，且大多数生成的题目包含参考答案，便于在教育实际场景中使用。同时，通过给出后续指令，可以对所生成习题的细节性信息进行修改，并可以生成多语种题目。由此可见，在教师教学的应用维度，系统初步具备了依据教学目标生成创作型教学素材的应用能力，可以辅助教师完成包括题目生成在内的多项具体教学任务。</p><p><strong>（二）自动解题</strong></p><p>如表2所示，用户输入一道涉及加法、除法、百分比等相关知识的数学问题，系统可以自动生成解答内容。在解答内容中，系统首先指出已知条件，即盐的质量为2克，然后解析盐和水的总质量为202克，进而给出质量占比的具体计算公式并计算出正确结果。最后，系统用规范的语句正面回答了该数学问题。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/Oa3Nicd3m3JMfHr6qNgBIHib4ZjV7K1jBqFJXzmCjLib8r94rE0Jhjcw4DQ7LRWC5aN7hr1GDjUEXPrxpXlrGsnGg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="表2 自动解题示例"></p><p>为更好地验证系统的自动解题能力，我们从Multiarith数据集（Roy &amp; Dan, 2016）中选取了50道习题组成测试集。Multiarith数据集是一个多步骤算法数据集，包含600道小学级别的情景类数学题。通过调用InstructGPT相关模型接口进行持续测试，我们发现系统解题的平均准确率约为68%。该结果已经明显高于GPT-3的准确率，且解答错误的原因大多是源于对情境中所需常识性知识的误判。我们进一步对系统解题过程中的推理文字进行分析，发现所生成的文字合理且易懂，大幅度超过了之前GPT-3的逻辑表达能力。例如，GPT-3生成的解题思路通常会存在错误的因果关系和推理，但ChatGPT系统已经可以给出完整清晰的解题逻辑，且在关键步骤上均配有计算公式和描述。</p><p>通过以上试验，我们可以初步验证系统已经具有较好的自动解题功能，所生成的解题结果具有一定的准确性与可读性，其逻辑表达清晰且形式丰富。由此可见，在学习过程维度，系统已经初步具备了完成个性化学科知识问答与支架式教学反馈等教育应用的能力。另外需要指出的是，对于具有较为复杂情境的题目，ChatGPT系统自动解题和反馈能力还有待提高。</p><p><strong>（三）辅助批阅</strong></p><p>如表3所示，用户输入一道完整的题目及其错误解答，系统可以根据用户指示，自动判断答案正误并给出具体反馈，从而完成辅助批阅的基本任务。在生成的辅助批阅文本中，系统首先明确给出了正确与否的判断，然后用合理准确的语言给出了错误原因的分析，即指出桃树数量“不是仅仅是苹果树数量的1&#x2F;5”，而应是“苹果树数量的1&#x2F;5加上苹果树数量”。在此基础上，系统可以继续自动给出合理的解题过程与正确的答案。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/Oa3Nicd3m3JMfHr6qNgBIHib4ZjV7K1jBqNuEW6eXhZbZ0wmuHJCnoN6kD89lVhiaMOBfqAaTgrDdw71gUVHsbluQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="表3 辅助批阅示例"></p><p>我们进一步对题目内容和答案进行更改并测试，发现系统可以持续输出类似的辅助批阅结果。由此可见，系统已经具有题目答案正误判断和错因分析等基本功能，即具备了对学生作品和习题答案进行客观点评和判断的能力。这种诊断性评价能力具有很高的教育应用价值，是构建全流程自动化教育评价服务的关键性技术保障和基础。</p><p>五、</p><p>局限与启示</p><p><strong>（一）局限与问题</strong></p><p>以ChatGPT为代表的生成式人工智能系统仍然面临诸多局限。首先，系统仍然难以充分理解信息和分析信息内在的逻辑关系，因此很容易生成不合理的内容或者犯事实性的错误。例如ChatGPT会非常自然地回答诸如“诸葛亮是如何打败秦始皇”或“林黛玉初见曹雪芹的情景”，所给出的错误且荒谬的答案体现了该技术并不能像人类一样完整理解知识体系与内在联系。这种事实性的错误也容易误导不具备专业知识或相关常识的学习者，引起学习迷航与认知障碍。</p><p>其次，生成式人工智能的过程仍然是黑箱，所生成的内容不具备可解释性与明确的依据。例如让ChatGPT 写出“低时间复杂度的字符串匹配”代码，即使所生成的长段代码可以运行且结果看似正确，但由于其中代码生成的来源和算法依据都不清晰，代码中细微的逻辑错误或步骤冗余也难以精确识别，因此难以直接应用于重要的课程实践和高利害的考试任务中。</p><p>另外，生成式人工智能在中文语境和文字上的理解和表达能力总体上要弱于英文。例如对于“苹果比梨多1&#x2F;6”这样含义的语句，ChatGPT经常会生成“苹果是梨的1&#x2F;6”或“苹果是梨的1&#x2F;6倍”这样的错误或不符合中文语言习惯的表述。这种能力差距源于预训练语料中以英文为主的现实情况，也因此可能导致非英语母语的学习者理解困难甚至理解错误。</p><p>同时，生成式人工智能技术也可能被恶意利用，造成较为严重的安全隐患。例如，可以用多步提示的方式，引导系统给出“如何入室盗窃”或“如何制造伤人工具”这类问题的危险答案。这些危险信息如果被各学段学习者获得和传播，会带来较为严重的青少年问题和社会危害。另外，生成式人工智能技术的数据源本身复杂且庞大，其生成的内容可能有知识产权问题，且容易产生法律风险。例如在模型训练和微调过程中，ChatGPT等产品所需的大规模数据集不可避免要涉及各类受法律保护的知识产权类数据，这些未经著作权人授权的数据的使用以及所生成的相应内容，存在侵犯他人著作权或专利权的风险。</p><p>最后，需要指出的是，以ChatGPT为代表的生成式人工智能虽然在教育领域有广泛的应用前景，但并非在所有场景都有重要应用潜力和作用。例如在教育智能化管理与服务中，通常需要依靠准确的数据支撑和透明的决策模型，很难简单依赖“黑箱式”的生成式人工智能技术。另外，生成式人工智能的模型训练、测试与下游任务适配，均需要较大规模的计算资源和存储资源进行支持，这种高成本对于相当一部分教育业务是难以承受的。因此，人工智能生成内容技术的教育应用范围和场景也有一定的局限性，应避免在教育领域盲目推广和普及。</p><p><strong>（二）启示与展望</strong></p><p><strong>1. 推进教育理念变革</strong></p><p>虽然以ChatGPT为代表的人工智能生成内容仍然存在诸多局限，但其所具备的核心能力已开始对教育理念产生直接影响和启示作用。我国现阶段教育仍重视通过大量记忆、识别和练习而获取知识，忽视通过分析思考而发现并掌握知识的方法与技能。生成式人工智能技术已逐步显现出高效积累知识与合理使用知识的基本能力，可以预见将替代和超越只能获取和存储知识的低阶思维脑力劳动者。因此，教育应该更加侧重于培养学生的高阶思维能力，尤其是跨学科多元思维能力、批判性思维能力与创造性思维能力。只有具备较强的跨学科多元思维能力，学生才能认识和区分现实世界的复杂问题和情境，并最终完成人工智能难以应对的实际任务；只有具备良好的批判性思维能力，学生才能对知识和技能有超越人工智能模型的深入理解和分析，并充分认识到人工智能技术的局限及其工具属性；只有具备一定的创造性思维能力，学生才能充分挖掘和发挥自身在特定领域的创新潜力和作用，避免被智能机器在专业领域简单替代。同时，新技术条件下的教育，需要加速教师队伍的观念转变，让一线教育工作者充分认识到技术变革所带来的社会需求变革，充分调动教师在教育理念变革过程中的积极性和创造力。</p><p><strong>2. 创新教学方式与内容</strong></p><p>在重视高阶思维能力培养的教育理念驱动下，生成式人工智能技术与产品对教学方式与教学内容的影响也会逐渐显现，并扮演不同的角色和发挥不同的作用。在教学方式上，需要鼓励教师积极创新课堂教学方式，将相关技术纳入不同学科的教学过程中，丰富课堂活动内容及其趣味性。例如，通过设置具备良好交互能力的人工智能助教，提供实时机器反馈甚至人机辩论环境，鼓励学生与机器助教开展共创性学习，持续性获取所需的个性化学习信息与资源，从而培养学生高阶思维能力和自主学习能力。在教学内容上，需要积极调整不同学科的培养目标和教学要求，更加强调学科核心素养导向的教学内容设置。例如当前人工智能生成内容技术已经具备良好的多语言代码生成与调试能力，初级程序员的社会分工可能将逐步消失。因此，对于基础教育与职业教育阶段的编程类教学，需要更加强调计算思维、人工智能素养与算法思维的培养，减少对于程序语言中语法细节的记忆性学习。</p><p><strong>3. 鼓励教育与技术互促共进</strong></p><p>人工智能生成内容相关技术的演进速度非常快。以GPT系列为例，从第一代GPT-1到目前的ChatGPT经历了四代更新，每一代的性能都有明显提升，但更新换代的时间不足五年。因此，可以预见更加智能化和人性化的生成式人工智能技术与产品将会在短期内出现，其在自然语言处理等任务上的性能将进一步提升，也将具备更优秀的内容理解、生成与泛化能力。因此，教育需要积极适应人工智能技术的快速发展，对其持有更加开放和包容的态度，鼓励教育工作者秉持技术向善理念，研究和使用相关技术和工具，协作完成各类教学任务。同时，需要充分认识这类新技术不再是“拍照搜题”或“换脸软件”，而可能成为未来教育的重要组成部分并对教育领域具有深刻的变革性意义。另外，教育领域也需要高度关注生成式人工智能技术的潜在安全与伦理风险，针对教育领域的应用场景，推进制定相关法律法规，形成技术与教育双螺旋式的互促共进。当通用人工智能已经逐步接近人类社会，教育作为人类文明进步的基石，应该从容应对挑战且充满自信。</p><p>作者简介</p><p><strong>卢宇，</strong>北京师范大学教育学部未来教育高精尖创新中心副教授。</p><p><strong>余京蕾，</strong>北京师范大学教育学部教育技术学院博士研究生。</p><p><strong>陈鹏鹤，</strong>北京师范大学教育学部未来教育高精尖创新中心讲师（通讯作者：<a href="mailto:&#99;&#x68;&#101;&#110;&#x70;&#x65;&#x6e;&#x67;&#104;&#x65;&#x40;&#98;&#110;&#117;&#x2e;&#101;&#x75;&#x2e;&#x63;&#x6e;">&#99;&#x68;&#101;&#110;&#x70;&#x65;&#x6e;&#x67;&#104;&#x65;&#x40;&#98;&#110;&#117;&#x2e;&#101;&#x75;&#x2e;&#x63;&#x6e;</a>）。</p><p><strong>李沐云，</strong>北京师范大学教育学部教育技术学院硕士研究生。</p>]]></content>
    
    
    
    <tags>
      
      <tag>ChatGPT</tag>
      
      <tag>生成式人工智能</tag>
      
      <tag>教育应用</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>周玲 王烽丨生成式人工智能的教育启示：让每个人成为他自己</title>
    <link href="/2023/06/09/5/"/>
    <url>/2023/06/09/5/</url>
    
    <content type="html"><![CDATA[<p>本文来源：周玲，王烽.生成式人工智能的教育启示：让每个人成为他自己[J].中国电化教育，2023，（5）：9-14.</p><p><em><strong>*摘要*</strong></em></p><p>人工智能时代，技术的革新给包括教育在内的社会各行各业不断带来深刻的影响。以ChatGPT为代表的生成式人工智能，成为推动人类进步的聪明的工具，给学校教育教学活动效率和质量的提升带来了机遇与挑战，同时也引发了对于培养什么样的人、如何培养人等教育本质性问题的反思。ChatGPT为实现学生全面发展、推行因材施教的个性化教学，以及重塑充满活力的课堂教学创造了更多的需求和可能。</p><p>关键词：生成式人工智能；ChatGPT；人的全面发展；个性化教学；课堂教学</p><p>人工智能时代正在加速到来。从围棋超能AlphaGo到日渐成熟的自动驾驶技术，从AI绘画到AlphaFold 2的蛋白质结构预测，越来越“聪明”的人工智能应用正在悄悄改变着我们习以为常的学习、工作和生活。2022年底开始风靡全球的生成式人工智能系统ChatGPT，就像一头“闯进房间里的大象”。</p><p>生成式人工智能（Generative Artificial Intelligence）是一种特定类型的人工智能，它可以通过人工智能相关技术，自动化生成文本、图像、视频、音频等多模态数据。ChatGPT全称为“Chat Generative Pre-trained Transformer”，直译过来的意思就是“基于特征抽取转换模型的预训练语言生成聊天工具”，依托的关键技术是机器学习、神经网络和特征抽取语义转换。以常人的视角看，它的工作程序是识别和理解指令、注意力机制信息检索和筛选、总结和归纳、反馈和表达，以及贯穿始终的学习进化。</p><p>最新的ChatGPT-4模型不仅可以用来聊天，可以根据上下文与人互动，撰写各种文案、论文，进行文学创作甚至写代码，还具有了强大的图像识别能力，问题回答的准确率显著提高。ChatGPT不像AlphaGo那样相对小众，而是开始参与多数人的日常；也不像工业机器人那样只淘汰产业工人，而是影响到了与知识服务相关的所有岗位。</p><p>ChatGPT一经推出，其用户数量增长方面达到了前所未有的高度，同时引发了社会的广泛热议。来自教育一线的教师和管理者、教育研究者、教育政策制定者都在力图对其价值和应用进行客观的审视：我们应以什么样的态度来面对技术革新？以ChatGPT系统为代表的生成式人工智能技术将对现有的学校教育产生什么样的影响？</p><p>1</p><p><strong>ChatGPT冲击：技术变革对人自身功能的替代</strong></p><p>生成式人工智能的教育启示：让每个人成为他自己</p><p>一部人类文明发展史，就是一部技术变革的历史。如果说科学探索在不断拓展着人的认知疆域，那么技术进步在加速这一进程的同时，对人自身的功能产生了持续的替代作用。技术作为卡尔·波普尔所谓“世界3”的组成部分，产生于人的智力活动而又独立于人存在，推动人类进化并随着人类而进化，并且越来越走向人类活动的中心。以机械化为标志的第一次技术革命替代了大量简单的体力劳动、解放了人类的身体，以电气自动化为标志的第二次技术革命开拓了人类活动新疆域、创造了更加丰富的物质精神生活；以信息化为标志的第三次技术革命加速了人类知识共享和交流、创造了客观虚拟世界。目前正在迅猛发展的人工智能技术是信息及技术发展的新阶段（也有人称之为第四次技术革命），基于信息技术整合有关技术成果，模仿人类认知活动和思维能力，开始替代并超越人类脑力劳动，开辟一条让人类从身体到大脑逐渐获得解放的路径，也带来关于人的新的思考和挑战。</p><p><strong>（一）ChatGPT是一种会学习和思考的“机器”</strong></p><p>ChatGPT模仿了人类分析、综合、比较、抽象、概括、判断和推理等基本的思维过程，而且在信息检索的广度、速度和分析归纳的效率等方面远超个体的人类。它根据指令对以文字开放存储的知识和数据进行特征检索和概括，可以承担很多繁重的案头工作，质量已经达到相当高度。最为引人瞩目的是它的学习能力，它不仅可以在聊天对话中学习，而且可以在检索中学习数据的产生方式，不仅在特定的任务中学习，而且可以在一定程度上实现自动学习，并能够实现不同任务间的学习迁移，几乎是一个不断快速进化的数字大脑。不可否认的是，“机器”越来越像人了。</p><p>随着自身的进化，生成式AI将逐步进入更为专深的工作领域，并引发深刻的模式创新和产业变革。在营销、金融等领域的已知应用场景非常丰富，而在工业设计、药物研发、材料科学等生产领域的应用潜力十分巨大[1]。</p><p>尽管ChatGPT对劳动力市场的冲击还没有真正开始，但是以它为代表的人工智能在越来越多的岗位上替代人类的现有劳动是必然趋势，完成特定的工作，使用它比用人的成本要低得多、效率要高得多。这既是一个技术性失业的老问题，同时又不同于分析式AI对低技能、低教育水平劳动者的替代，以ChatGPT为代表的生成式AI很可能替代那些教育程度较高、技能较强，同时薪酬水平也较高的中高层白领，如会计师、市场研究分析师等[2]。美国近期一项研究结果表明[3]，大约80%的美国劳动力可能至少有10%的工作任务受到ChatGPT的影响，而大约19%的工人可能会看到至少50%的任务受到ChatGPT的影响。这种影响跨越所有工资水平，其中高收入工作可能面临更大的风险。值得注意的是，这种影响不仅限于近期生产率增长较高的行业。ChatGPT可能带来显著的经济、社会和政策影响。</p><p>每一次的技术进步，都在不断提高人类的生产效率，赋予个体更多追求有意义的闲暇生活的机会。认识到这一点，让我们不会对新技术产生恐慌，让我们能够理性地认识和接纳新技术，并对人类生存和发展的意义进行更深入的思考。</p><p><strong>（二）ChatGPT是一款聪明的工具</strong></p><p>ChatGPT很聪明，但相对于“人”来说，它还是极为简单和初步的。它只是一个专业性人工智能应用，尽管它的工作过程与已知的人类思维过程极为相似，但是它的“思维”是建立在人类已有的概念和判断基础之上的，是对已有知识的特征性筛选、统计和逻辑演算。ChatGPT作为工具的局限性，体现在[4]：</p><p>其一，它不能凭借对现实世界的感官体验在“实践”中形成原创性的知识，不能经历从实践中来、到实践中去的认识升华过程，它的学习进化也只是工作技能和质量的进化，它没有创造性，更不会实现智慧的明悟。</p><p>其二，它以输入-输出方式工作，可以根据上下文关系将聊天持续下去，而且已经可以识别分析长文本输入指令，但一直是被动应答，不能主动提出问题并通过平等对话启发提问者的思路，只能“言传”而不能“意会”。它不能借助没有答案的问题启发人的思维，也没有独立思考能力。</p><p>其三，它模仿了神经网络工作原理，通过“注意力”机制搜寻信息、产生联想和实现链接，但还不能复制实际上千差万别的人的思维方式，更不能模仿人类自身也还尚未彻底揭开的思维的秘密。</p><p>其四，它没有人的情感、人的意志品质，不能与人进行情感交流与共鸣，不懂得爱和被爱，没有作为人的社会关系和个体经验的积累，缺乏独特性和个性。</p><p>因此，ChatGPT本质上是一个工具，一个高级工具，而不是一个生动的、具有自我意识的个体，更不是一个能够自主发展的人。</p><p><strong>（三）ChatGPT给人类带来新的不确定性</strong></p><p>技术的发展不断拓展人类作为一个整体的能力，创造新的未来。新技术总是依靠商业价值引领和市场开拓，形成新产品、新业态、新模式。新的生产方式代替旧的生产方式，新的生产关系就会代替旧的生产关系。ChatGPT在带来令人兴奋的应用前景的同时，也像一个横空出世的挑战者，给人类生产、生活和社会秩序带来的冲击还无法准确预判。</p><p>最近，包括埃隆·马斯克在内的多位知名企业家、包括信息技术和AI开发领域在内的一批领军科学家共1100多人联合签署了一份关于暂停巨型人工智能实验的公开信，他们担心具有与人类智能竞争性质的人工智能系统给社会和人类带来的风险。这表明，人类包括系统开发者自身，都还无法保证未来人工智能产品的正义性和安全性。随着人工智能产品的不断升级和快速迭代，它似乎超越了人类的适应能力。而个体人类的思考能力在巨型人工智能系统面前是弱小的，人类多样化和个性化的学习和思考方式，会不会被这类产品挤压和同化；人类的情感、价值观，会不会在“技术为王”的环境中被物化或它化？技术的进步的可控性，依赖人类共同的人文价值和新的制度规则。</p><p>2</p><p><strong>ChatGPT启发对教育根本问题的追问</strong></p><p>生成式人工智能的教育启示：让每个人成为他自己</p><p>“培养什么样的人”是教育的“原问题”，是习近平总书记反复强调的“教育的首要问题”。人的全面发展理论是马克思主义经典理论，也是我们党的教育方针的重要来源。人的发展最重要的制约因素是生产方式和生产关系，马克思认为，资本主义机器大工业为人的全面发展创造了条件。第一次工业革命后机器开始大规模淘汰手工业者，信息技术和自动化技术淘汰生产线上的工人，ChatGPT和其他人工智能则有可能淘汰提供一般性知识服务的岗位，创造不同的产业模式。人工智能在短期和表面上影响就业的同时，实际上在重新撬动社会分工，特别是人和“机器”的分工。可以预测，ChatGPT能使更多的人从繁重的信息检索、统计和文案工作中解放出来，投入到知识技术含量更高、更加突出个性化和创造力的岗位。技术带来劳动生产率的极大提高和财富极大增加，人将有更多的时间学习、休闲，实现全面而自由地发展。</p><p>ChatGPT可以根据学生要求，向它们呈现对现有知识的提炼，但是不能代替学生通过具体实践获得经验的过程，不能使学生获得与现实场景和真实人交往交流的感受，也不能像父母、亲友、老师、同学一样在交流互动中给予学生各种情感体验。ChatGPT可以提高学生知识学习的效率，但不会像学校、家庭一样帮助学生成为一个社会化的人。ChatGPT的出现从反面告诉我们，过去曾被忽视的生活教育、劳动教育、实践教育、社会情感培养、爱的教育等等，在学生“成人”中的极端重要性；对学校和教师来讲，与学生家庭、社会一起开发教育资源，创设教育情境，对学生实施价值引导、情感培育、创造力培养、习惯养成、生涯指导、心理疏导等，比传授书本知识更为迫切，应当成为教育的核心。</p><p>从处理文本知识的表现来看，ChatGPT就像一个现实中的“学霸”，按照现在的学业标准特别是考试要求，只要进行一些技术升级，它的表现会比所有人更为优异，这使得学生之间围绕考试分数的残酷竞争看上去就像个“笑话”。的确，ChatGPT像一面镜子，映照出我们教育的弊端。正如有学者指出的，“机器越来越像人了，我们还在把人当作机器培养”。假如这种教育模式持续下去，个人的发展将永远跟在人工智能后面，而且将被甩得越来越远。教育的重心将向培养学生独立思考能力、创造性思维、批判性思维、情感、同理心、价值观以及人际交往等能力转移，注重和提升学生的身体健康和心理健康水平。真正点燃每个孩子内心的火种，让每个人都成为他自己。让人永远高于机器，成为时代赋予教育者的重大使命。</p><p>因此，教育应该进一步突出人的个体需求和社会关系属性，淡化人作为生产工具的属性，应更加注重于服务工作变换和人的全面自由发展，这样，促进学生“德智体美劳全面发展”就有了更加坚实的社会基础。另一方面，ChatGPT挑战的是以应试为目的的“智育”模式，同时为学生全面发展创造更多的需求和可能。</p><p>3</p><p><strong>ChatGPT推动人文主义教育方法回归</strong></p><p><strong>‍</strong></p><p>生成式人工智能的教育启示：让每个人成为他自己</p><p><strong>（一）为个性化教学的实现提供技术支撑</strong></p><p>可以肯定的是，ChatGPT的出现并不是否定知识学习的重要性，人的一切高级能力的发展都是建立在知识基础之上的，关键是在知识爆炸的时代，如何让每一个学生都能找到适合自身特点的学习之路。</p><p>有研究从教师教学、学习过程与教育评价三个维度，分别选取题目生成、自动解题与辅助批阅三个具体教育应用，对ChatGPT系统进行初步验证[5]。结果发现，ChatGPT系统可以持续生成质量和难度均适中的多学科、多情境习题、解题步骤及参考答案，还能对习题答案正误进行判断和错因分析。</p><p>可见，现有学校系统中部分侧重在单一学科知识点传授、练习、评价等环节的工作可以借助生成式人工智能技术完成，教师可以将更多的时间和精力投入到教学设计中。在了解学生的已有经验与学习特点的基础上，教师可以有针对性地构建个性化、差异化的教学系统，借助人工智能技术，从教学目标的设定入手，在学习过程中给予学生适当的自主权和控制权，并通过及时的反馈和学习策略支持，实现同一教学环境中不同个体有针对性的差异化教学，真正实现因材施教。此外，面对那些学习上面临挑战的学生，人工智能技术的运用，将为我国全面推进融合教育提供技术上的抓手和支持。</p><p><strong>（二）为思维能力培养范式的转变带来机遇</strong></p><p>按照布鲁姆对认知领域教学目标的分类，记忆、理解、应用属于低阶思维，主要用于对具体知识、普遍原理、概念知识等的掌握；而分析、综合、评价是更高层次的心智活动，包括关系分析、推导、判断等认知能力。以ChatGPT系统为代表的生成式人工智能技术的应用，将改变教学过程中对不同水平思维能力培养的范式。“从低阶到高阶”的传统模式注重低阶思维的训练，尤其强调记忆的前提作用，而生成式人工智能技术突破了人类客观知识的记忆广度与深度，学校教育将更加注重学生高阶思维能力的培养，建立“高阶低阶融合贯通”新模式将成为推动教育变革的着力点。</p><p>同时，ChatGPT也将有助于提升教学实践过程的创造性，如果ChatGPT能够得到适当的、有效的应用，可促进“人工智能素养”本身成为任何科目中的融入性教学目标；教师可以利用ChatGPT培养学生的批判性思维。比如，教师可以要求学生分析ChatGPT生成的关于历史事件的报告，追踪其信息来源，并评估信息的真实性与有效性；教师还可鼓励学生质疑ChatGPT所提供信息中的逻辑推理以学习修辞学；还可利用ChatGPT培养学生的问题意识与提问能力等[6]。</p><p><strong>（三）为个体经验的整合创设机会</strong></p><p>杜威说：教育即生活，教育即生长，教育即经验的改造。教育本身是“活”的，已有知识的学习应当与活的教育结合起来，纳入学生的认知结构，形成与其他人不同的个体经验，让每个人成为不同的人。</p><p>现实世界的复杂问题和情境不是为单一的学科知识准备的，解决这些问题和应对这些情境，需要个体运用多学科的知识和对经验的整合。ChatGPT系统既为培养学生跨学科的多元思维能力提出了要求，也为其创设了条件和机会。</p><p>首先，帮助教师准确地掌握学生的学习状况。利用人工智能技术对学生在学习过程中的学业成绩数据、行为表现数据、生理和心理发展数据等进行全方位的收集与智能化分析，以对学习者的学习动机、学习风格、自我效能感等认知与社会性发展的特质充分了解，在此基础上，结合个体的经验建构学生的个性化发展路径，为学生个性化的学习服务供给提供参考。</p><p>其次，帮助学生在真实的世界中构建复杂多元的发展背景。利用多模态数据构建丰富的认知空间，从对现实中综合性问题的感知入手，这样的问题是学生面对的真问题，学习掌握信息的辨别、判断等基本技能，依照不断生成中的价值图式，渐进式地并富有成效地解决个体发展中的矛盾冲突。</p><p>4</p><p><strong>ChatGPT加速教与学关系的变革</strong></p><p>生成式人工智能的教育启示：让每个人成为他自己</p><p>作为一个语言模型，ChatGPT使用的并非全新的技术，它只是一个应用已有技术的商业产品。这个产品并非专为教育而研发，它没有情感和记忆，也没有为学生创建特殊应用场景。</p><p>在辅助教师生成个性化教学方案的过程中，ChatGPT等系统可以依据教师的教学需求，分步骤生成多种适切的教学设计，为教师在备课过程中提供思路启发与多种备选方案[7]。它能帮助学生和教师获得准确、有用的信息和知识，回答他们提出的问题，使知识学习和教学工作变得简便。它的出现表明了神经网络、机器学习、Transformer等人工智能技术支持学习模式变革的确定性，如根据学生的学习成绩、兴趣爱好、学习习惯等数据，自动生成学生自适应学习的路径；帮助在线学习平台记录学生的学习过程和历史数据，自动推荐适合学生的学习资源；通过公共学习平台或其他学习管理系统监测学生的学习进度、活跃度、答题情况和错误率等数据，对学生进行实时诊断和评估；以及为每个学生提供学习账号，并在此基础上定制不断进化的、专属的、个性化的学习助手等等。这些都为我国各级智慧教育平台的优化升级提供了现成的思路和技术支撑，同时也提出了重塑课堂教学的挑战。</p><p>在课堂师生互动的教学过程中，教师在教学中始终处于主导地位。在知识更新迭代加速与海量信息深化的人工智能时代，教师主导性的发挥将更多地体现在“生命的示范性”[8]。</p><p><strong>（一）重视教师自身的主体性成长</strong></p><p>教育是富有生命力的人与人之间、师与师之间、师与生之间、生与生之间的互动活动，智能时代，教师作为社会人和职业人的主体性成长都面临着极大的挑战。技术持续“高位发展”的境况中，“师道尊严”的权威身份被挑战，主体地位岌岌可危，教师陷入他者承认和自我认同的焦虑中[9]。让每个人成为他自己，教师也不例外。</p><p>技术的进步能够帮助教师摆脱以往简单、机械、冰冷冷的教育言行，教师富有情感的鲜活互动是任何技术都替代不了的。培养全面发展的学生，教师也应是全面自由发展的。丰富教师从教的责任感、使命感和幸福感，使“经师”与“人师”融为一体，培育学生的生命自觉，成为学生生命的守望者[10]；激发和调动教师的内在学习动机，帮助和支持教师保持持久的学习动力，成为学生学习的示范者。</p><p><strong>（二）做好技术伦理的教育与示范</strong></p><p>40—45分钟的课堂时间不再主要用于知识的传授，翻转课堂等新型课堂教学模式将在ChatGPT的帮助下不断增强适用性。借助ChatGPT等生成式人工智能技术，学生将有机会接触海量的信息，如何判断信息的真伪、如何提取与问题相关的信息、如何既合乎逻辑又充满情感的分析信息，以得出结论，技术素养的养成是智能时代课堂教学的重要内容。智能时代学生的媒介素养不仅仅应包括学生对于智能技术的掌握与使用，还应该囊括学生正确使用智能技术的知识、运用智能技术的能力以及区别与智能技术的综合能力[11]。</p><p>作为智能技术应用的主要推动者、使用者以及把关者，教师智能技术教学应用的伦理诉求状况决定了教与学的效果与品质。教师在将智能技术运用到教育教学过程中时，需遵循智能技术教学应用伦理，它是属于教育技术伦理范畴，是教育伦理、教学伦理与技术伦理相交叉产生的新型伦理形态[12]。教师既要注重自身技术伦理的示范性，还要加强对学生技术伦理的价值引领，例如专门向学生传授有关技术伦理的相关知识，防止学生将ChatGPT作为减少学习投入的工具，防止学生在技术运用过程中的失范行为。</p><p><strong>（三）创建有温度的人际交往空间</strong></p><p>从学生个体的自我意识发展来看，智能技术过度依赖会导致学生的精神发展受阻，注意力、意志力、想象力等日趋浅薄。在未来人机交互的过程中亟需关注学生情感发展，积极创设富有情感温度的人智交互模式。</p><p>课堂教学中的人际互动，包括行为互动、知识互动、情感互动等。智能时代，技术在知识互动中发挥了重要的中介作用，表现出“师-机-生”的互动模式，而教师与学生之间，以及学生与学生之间的情感互动则是技术难以替代的。课堂教学中的人际互动不仅具有现实存在价值，还具有重要的教育和成长价值。例如近年来广受关注的社会与情感能力（Social and Emotional Competence），作为一系列识别、管理、应对人际互动中情绪情感的能力，不仅直接影响到个体的心理健康和幸福感，也是未来工作场域人才需求的重要技能，对工作绩效产生重要影响。智能时代课堂教学中，社会与情感能力的个体和社会价值更加凸显，教师可专门教授社会情感技能，并在课堂人际交往中实践社会情感技能[13]。</p><p>5</p><p><strong>结语</strong></p><p>生成式人工智能的教育启示：让每个人成为他自己</p><p>《中国教育现代化2035》提出了推进教育现代化的八大基本理念：更加注重以德为先，更加注重全面发展，更加注重面向人人，更加注重终身学习，更加注重因材施教，更加注重知行合一，更加注重融合发展，更加注重共建共享。生成式人工智能技术的进步将助力实现教育现代化的发展目标。</p><p>教育是培养人的活动，是教育者与受教育者借助教育媒介建立的复杂的互动关系，是人的成长发展过程。教育在人的复杂性之上增加了发展的维度，因而更为复杂。相对于教育，ChatGPT同样是简单和初步的，它充其量是一个很有效的辅助工具。目前关于它会给教育带来怎样的冲击的讨论，有一些不免言之过早；至于说它将会淘汰教师职业，更是言过其实了。然而，这些讨论仍然具有深远意义，毕竟ChatGPT和其他人工智能产品一起，正在打开技术变革引发教育模式变革、推动教育制度变革的路径。</p><p>科学技术的重大进步，通常都会带来新的伦理和安全问题。ChatGPT和其他人工智能产品用于教育，涉及未成年人的隐私保护问题，越是贴心、个性化的产品，其掌握个人隐私信息也会越多，建立相应的技术标准，完善政府监管、行业自律、法律规范制度越来越迫切。</p><p>面对变革，我们只能主动适应，否则将会被淘汰。未来，人工智能还会朝着能力综合化、服务个性化的方向发展，出现与人更为接近的智能机器人，更严峻的挑战还在后面。也许更远的将来，它们会具备人格，与人类成为朋友或者同事，共同接受教育，并能够跟人类一起改造我们的教育。</p><p>参考文献：</p><p>[1][2]陈永伟.超越ChatGPT：生成式AI的机遇、风险与挑战[J].山东大学学报（哲学社会科学版），2023，（3）：127-143.</p><p>[3]Tyna Eloundou，Sam Manning，et al.GPTs are GPTs：An Early Look at the Labor Market Impact Potential of Large Language Models[EB&#x2F;OL].https：&#x2F;&#x2F;doi.org&#x2F;10.48550&#x2F;arXiv.2303.10130，2023-03-23.</p><p>[4]王烽.当ChatGPT、人、教育三者相遇[J].中小学管理，2023，（3）：29-30.</p><p>[5][7]卢宇，余京蕾等.生成式人工智能的教育应用与展望——以ChatGPT系统为例[J].中国远程教育，2023，（4）：24-31，51.</p><p>[6]郑燕林，任维武.实践观视域下ChatGPT教学应用的路径选择[EB&#x2F;OL].https：&#x2F;&#x2F;kns.cnki.net&#x2F;kcms2&#x2F;article&#x2F;abstract？v&#x3D;3uoqIhG8C45S0n9fL2suRadTyEVl2pW9UrhTDCdPD67lSnTj6ObN9AamOM996lj3fTRq0M_xc3SDl26DofFnUnD9G7zTSFw2&amp;uniplatform&#x3D;NZKPT，2023-03-30.</p><p>[8]王红霞.教师生命理想的缺失与重建[J].教师教育研究，2017，（6）：16-22.</p><p>[9][10]孙瑞芳，腾洋.人工智能时代教师主体性的遮蔽与复归[J].教育研究与实验，2023，（1）：52-59.</p><p>[11]岳伟，闫领楠.智能时代学生主体性的异化风险及其规避[J].中国电化教育，2023，（2）：90-97.</p><p>[12]卢佳，陈晓慧等.智能技术教学应用伦理风险及其消解[J].中国电化教育，2023，（2）：103-110.</p><p>[13]杜媛，毛亚庆.基于关系视角的学生社会情感能力构建及发展研究[J].教育研究，2018，（8）：43-50.</p><p>作者简介：</p><p>周玲，副教授，硕士生导师，研究方向为教育经济与管理；</p><p>王烽，研究员，博士生导师，研究方向为教育领域综合改革、民办教育政策、考试招生制度改革、教育战略规划。</p><p>*本文仅用于教学，如果分享内容侵犯您的版权或者非授权发布，请及时与我们联系，我们会及时删除。</p>]]></content>
    
    
    
    <tags>
      
      <tag>ChatGPT</tag>
      
      <tag>生成式人工智能</tag>
      
      <tag>教育启示</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>杨欣 ｜基于生成式人工智能的教育转型图景——ChatGPT究竟对教育意味着什么</title>
    <link href="/2023/06/09/4/"/>
    <url>/2023/06/09/4/</url>
    
    <content type="html"><![CDATA[<p>本文来源：杨欣.基于生成式人工智能的教育转型图景——ChatGPT究竟对教育意味着什么[J].中国电化教育，2023，（5）：1-8+14.</p><p><em><strong>*摘要*</strong></em></p><p>在ChatGPT掀开的科技盛宴中，生成式人工智能可以启发的远不只是教育转型的方法选择、技术方案和应用场景，还包括它作为“智慧工具”可以为教育擘画的转型图景。一方面，基于生成式人工智能的教育转型之所以令人向往，关键在于它能赋予教育人机协同、知识突破、向下兼容、向“智”迭代的机遇。另一方面，从生成式人工智能暗含的弊端来看，围绕它开展的教育转型将会面临来自人机冲突、知识魔法、数字鸿沟和超脱现实的挑战。相应地，为了彰显“化挑战为机遇”的转型向度，教育中人有必要批判性地检验基于生成式人工智能的教育转型，而不是盲目地接受与欣赏这场关乎效能革命的科技盛宴，以使之讲述为一场敢于坚持公平的精彩冒险。</p><p>关键词：人工智能；ChatGPT；教育转型；AIGC</p><p>1</p><p><strong>ChatGPT究竟对教育意味着什么</strong></p><p>基于生成式人工智能的教育转型图景——ChatGPT究竟对教育意味着什么</p><p>ChatGPT的横空出世掀起了跨领域、跨学科的生成式人工智能（AI-Generated Content，以下简称AIGC）热潮，教育亦不能例外。在ChatGPT确实改变教育之前，它究竟对教育意味着什么自然给人留下了无限的遐想空间。这既是当下相关研究方兴未艾的理由，也让开放而不失审慎的研究态度显得尤为必要。事实上，无论是考虑到ChatGPT高昂的经济成本与苛刻的技术条件，还是它的效用边际与价值局限，亦或其之于教育利弊同在的复杂事实。人们都有理由相信，AIGC离教育仍有一段不可谓不远且有待观察和检验的距离。即便这个过程未必如当初印刷机开启现代教育一般漫长而又充满机械味，但也正是由于它不会重复印刷机与教育之间的现代故事，如今教育也难以沿用过去的规律对其予以解读和把握。所以，在ChatGPT掀开的科技盛宴中，AIGC究竟会为教育擘画怎样的转型图景也就构成了各花入各眼的开放议题。毕竟，除了“ChatGPT能为教育做到什么”这样的技术关切之外，“教育从它身上知道了什么”亦具有同样的思想价值。反而在告别了ChatGPT之于教育的技术诱惑以及由此带来的急迫心态之后，它对教育的价值更有机会得到全面的展现。特别是在教育数字化转型日益成为全球共识的时代背景下，ChatGPT可以启发的远不只是教育转型的方法选择、技术方案和应用场景，还包括它作为“智慧工具”可能在思想层面赋予教育转型的新机遇和新向度。这一切就像作为变革动因的印刷机，它不止在技术上创新了教育的目的、对象和方法，它还在思想上改变了十六世纪之人对何谓教育的看法，乃至让文艺复兴拥有了通过教育促进普通人智慧觉醒的兴趣和信念[1]。</p><p>要知道，在ChatGPT之前，那些为人所熟知的Deep Blue、Watson与Alpha Go等人工智能应用，它们往往是通过与人类的竞争来制造热点，进而以超过人类智慧的结果或标题获得广泛关注。与之不同，ChatGPT之所以能激发全球用户的参与热情与广泛讨论，既不是因为它这一次又具体战胜了谁，也不是它让人再次感受到了智慧上的威胁，而是与那些承诺媲美甚至超越人类智慧却只能停留在博弈游戏或者实验室的人工智能相比，ChatGPT作为具有独特优势的智慧工具不仅在人类日常生活中展现出了匪夷所思的能力和效率，更因此让人真切意识到“即便人类在逻辑推理、信息处理和智能行为领域的主导地位可能因为人工智能而不复存在，但这却不是让人感到沮丧和绝望的理由，因为人工智能也能把人类拉伸到在前所未有的智慧起点之上”[2]。换言之，正是越来越多的人因为ChatGPT而感到自己在智慧上的不足被弥补、优势被强化乃至起点被拔高，所以他们才会情不自禁的想使用它。更进一步来讲，这种智慧工具的独特在于，它不再强调人与机器（技术）的竞争，而是关注由人机协同开启的良性循环。这种智慧工具的优势在于，它可以通过尽可能多的掌握不同学科和领域的知识，然后去跟不同基础和背景的人进行对话交流，帮他们进行各种验证、论证和计算，以便使之有更多时间、精力、机会和条件去从事那些需要创造力的挑战性工作，最终反过来形塑不同以往的人类智慧。据此而言，基于AIGC的思想演进可以概括为，借助与人有着本质不同的智慧工具，横向突破人类的知识壁垒，向下兼容人类的基础与不同，以期实现人类智慧的向前迭代。</p><p>必须指出的是，即便AIGC预示着某种思想演进，但若要据此使教育转型从愿景走向实景依旧面临诸多如鲠在喉的挑战。究其实质，AIGC助力教育转型仍然遵循以下数字化路向，即“从范围更广、规模更大、质量更高的数据出发，然后依托更新的算法和更强的算力对数据进行组合与分析，以求实现与人类的常识、认知、需求、价值观相一致的教育转型”[3]。但这样的教育转型，既可能与人类的理解、判断和决策构成冲突；也可能因为数据的占有不同与质量高低而陷入数字鸿沟之中；更有可能因为总是从已有数据出发，所以它既无法呈现没有数据的道理，也不可能向没有数据的“现实”开放。对此，用大语言学家乔姆斯基的话来形容便是，“ChatGPT的本质只是从已有存量文本和数据中寻找规律，然后按规律来生产文本，是个‘高端的剽窃系统’”[4]。以此推之，人们从ChatGPT中体验的所谓“智慧”，有可能只是一种切换注意力的“知识魔法”：它完全有可能是对已有知识的巩固、延续或者毫无意义的胡乱堆砌，只不过是换成了重新注意的过程。由此带来的糟糕教育结果则是，使人如同欣赏魔法一般，纯粹因为看不透反而充满了无知的偏狭、盲目与热情，以至于遮蔽了人们对教育的真实见解[5]。而要应对此类挑战，把握好基于AIGC的教育转型向度就显得颇为重要。</p><p>2</p><p><strong>基于生成式人工智能的教育转型机遇</strong></p><p>基于生成式人工智能的教育转型图景——ChatGPT究竟对教育意味着什么</p><p>从AIGC的独特优势来看，教育转型之所以令人期待和向往，关键在于AIGC有益于推动教育在思想层面走向人机协同、知识突破、向下兼容、向“智”迭代。</p><p><strong>(一) 人****机协同</strong></p><p>从ChatGPT的成功经验来看，教育领域的人机协同已经具备了必要的技术基础。它意味着，在智能助手、智能伙伴、智能导师的加持下，构建教育与技术之间的创造性伙伴关系：随着AIGC在识别、理解、推理和判断上的能力增强，教育不仅越来越有可能摆脱自身的迷思、偏狭和成见，也越来越有机会开拓视野、丰富内涵与创新体系。反言之，对现有教育体系而言，“根据机器需要先将个人选择简单化与标准化，进而为了配合机器把个人选择和创新予以排除的现代化思路”[6]也就被置于了格外刺眼且必须予以消解的境地。一方面，机械教育即便还能在相当程度上发挥作用，但这样的教育体系是否足以承载可持续的人类未来却显得日益可疑[7]。另一方面，“尽管很多人们熟悉的工作岗位正在消失，但人类对教育的需求却从未止步，反而还在增长。这也意味着，教育的本质需要被反思和改变。在一个知识可以按需提供的数字世界，人们需要重新思考自己需要知道什么，以及如何知道”[8]。更何况，若是没有AIGC，那么这种以机器为先的教育体系不仅能被视作“情有可原”，类似地，人机协同也难以成为多数人想象教育的起点和参照。但是，AIGC的强势崛起表明，人机协同不仅随时可以从实验室走向了教室，还能轻而易举地构成不容回避且更加为人所接受的教育趋势。在此情形下，教育若再不做好人机协同的思想准备，它失去的绝不仅仅是那些基于AIGC的技术、方法或者应用，还包括那些与AIGC密切相关的高附加值就业机会。一言蔽之，AIGC推动教育走向人机协同的机遇在于，它将迫使人们站在更为广阔、开放的视角超越一味迁就机器的传统教育观念，换之以教育与技术的创造性伙伴关系，以便使人类因为教育保有可持续的未来。</p><p><strong>(二) 知识突破</strong></p><p>AIGC可以被视作知识变革的动因，显然不是因为它能提供那些不够确切的知识，而是因为它能从根本上突破自印刷机诞生以来的知识分化趋势。随着印刷机源源不断地将各种书籍分门别类地投入到图书馆之中，教育就已经处于了因为知识分化而形成的狭隘视野之中。“在这种狭隘的视野下，专家们一方面对越来越细小的事物了解得越来越深入，直到他们对原本一无所知的事情了如指掌。然后，他们成了教授，而他们所教授的课程则成了障碍，因为它难倒了那些没有准备好面对复杂专业的学生”[9]。与之同时，伴随着教育领域日益复杂且不断加速的大数据趋势，人们对知识分化的担忧又被进一步恶化[10]。历史地看，尽管知识分化可能淡化自身价值亦或构成人类认知挑战的弊端已经是如此明显，但多数时候人们却没有明显地感受到这类焦虑，这其中一个关键原因便是人们既能创造更精细的知识，也能创造与之相匹配的知识过滤系统[11]。无论是专家与专业机构，还是教师和图书馆都可视作过去的知识过滤系统。只不过到了今天，仅凭个体或者组织之力若要对抗知识分化的弊端，亦或从无数碎片化的知识中发现可为教育所用的知识，已经变成了某种天方夜谭。在此局面下，可以视作人类知识“阿基米德点”的AIGC便应运而生。它不仅可以在把教育的知识想象从“人类”与“组织”身上托举起来以俯瞰自身，还能站在超越人力极限、组织局限的视角重新审视知识分化的意义及其出路，更将使因为分科教育、分级教育、分类教育而在知识层面出现的“低水平复杂化”“圈层内神秘化”得到必要的澄清。如此一来，AIGC推动教育实现知识突破的机遇可以归结为，当这种如同从宇宙俯瞰地球的视角打开教育的知识视野之后，知识分化非但不会失去它应有的意义，那些被人忽略或者难以顾及的碎片化知识也拥有了可以被人理解的可能。更重要的是，代表着数字时代知识过滤系统的AIGC，能够藉由它对知识的重新获取、鉴别、筛选、储存、整合以及推送，从根本上打破因为知识分化而形成的领域壁垒与科层藩篱，以及那些由“小圈子”“小团体”利用知识不对称所设下的迷局和障碍。</p><p><strong>（三）向下兼容</strong></p><p>当AIGC头顶颠覆性技术的光环出现在世人面前时，它不仅能收获教育的猎奇心，也能满足教育对创新的需要。但问题是，如此颠覆性技术既能固化教育既有格局、重复过去的教育故事，也有机会引领教育破局乃至呈现不一样的精彩。前者意味着，AIGC只是让教育的强弱显得更加分明、在技术上的喜新厌旧更加明显，亦或令教育问题解决方案重复“旧瓶装新酒”的窘境——“正因为人工智能代表着‘先进’，所以无论旧方案是否有用，亦或它有着怎样的缺陷，这样的教育都会倾向于用人工智能把旧方案包装一遍”[12]。后者意味着，AIGC可以通过向下兼容教育的弱者、旧技术和老方案，改变教育的强弱格局与技术偏好，乃至让那些因为技术不成熟而被搁置的老方案重新焕发光彩。与前者相比，后者不仅更契合当下教育转型的公平追求，也指向了更可持续的教育未来，更与当下GPT模型应用趋势拥有异曲同工之妙——在全球范围率先使用GPT4.0模型的Be My Eyes就是一个致力于为超过2.5亿的盲人或低视力者群体服务的应用程序。</p><p>向下兼容作为计算机术语，是指用旧的版本程序创建的文档或系统仍能在新系统中被正常操作或使用。据此来看乔姆斯基对ChatGPT涉嫌“抄袭”的指责，不也正是因为它可以把那些散落在网络上专属于某些人（机构）的已有知识或者单一答案重新加以整合和运用。这些知识于乔姆斯基而言可能乏善可陈，却可能为普通人打开不一样的知识世界。换言之，这既意味着AIGC在知识产权上有待完善之处，也是它赋予教育转型的不一样精彩。它意味着，无论使用者、提问者是谁，AIGC都能为之匹配答案；无论是黑板、粉笔与书籍，还是多媒体教室、计算机、网络，它们都可以藉由AIGC的大数据整合、大算力支持和大模型分析，构建目标相互衔接、对象相互联系、内容相互协调的教育转型共同体；无论是远程教育、教育信息化、“互联网+教育”、翻转课堂、智慧教育这样在历史演进中证明过自己的问题解决方案，亦或那些曾经被人设想却苦于没有技术支撑的教育方案，都有可能藉由AIGC转化为教育的有效场景、核心应用、关键策略。这种向下兼容的机遇在于，如果AIGC可以构成兼容弱者、旧技术和老方案的“救命草”，而不是淘汰它们的“催命符”，那么它也将使教育转型因为“延续历史、盘活存量、涌现增量”而实现破局。因为基于AIGC的教育转型并不是纯粹为了引入新的胜利者、技术变量和问题解决方案，而是要试图赋能于弱者、旧技术和老方案，并盘活教育领域的已有因素，进而激活教育生态，使之不断迈向更高水平的平衡。也因为有了向下兼容作为思想基础，基于AIGC的教育转型不仅能得到更多参与者的响应和支撑，也能与其他技术支持下的教育转型在政策、实践和理论上构成世代更替、相得益彰的趋势，更有利于形成选择丰富且经得起时间考验的数字化教育问题解决方案。</p><p><strong>（四）向“智”迭代</strong></p><p>历史一再表明，现代人比洞穴人具有更为精密复杂的认知能力，不是因为现代人更聪明，而是他们制造了更为聪明的工作环境[13]。同时，如今教育能冠之以“智慧”之名，正是因为人工智能正在让教育朝着更加智慧方向进行迭代[14]。尤其是，来自神经科学、物理学、统计学、进化论、计算机科学的证据足以表明，善于学习的AIGC还可以在未来做得更多与更好[15]。以此观之，AIGC带给教育转型的最大礼物便是如同文字、纸张、印刷机一般，推动教育朝着更为智慧的方向进行迭代。要知道，“死记硬背”之所以在当下成为了代表“不智慧”的教育贬义词，不正是因为人们拥有了纸、笔、书籍以及电脑，所以仅凭记忆就能在教育中获得智慧名声的日子才会一去不复吗？当然，这并不是说记忆不再具有教育价值，而是想表明，当人类的认知因为技术被改变之后，人们不仅会形成全新的智慧见解，可以通过教育予以凸显的智慧潜力也将得到极大拓展。比如因为技术的进步，那些即便记忆能力一般，但善于其他认知技能的学生也可以在教育中获得属于自己的智慧荣光。所以，人们才会说如今的教育比过去的教育更为智慧。</p><p>源于此，基于AIGC的向“智”迭代便可归结为，推动教育朝着更智慧的方向进行改善、改变与重塑。第一，改善教育。从教育面临的突出问来看[16]，AIGC有机会对社会阶层固化、教育制度僵化、教师行政负担过重、大班教学抑制学习的个性化以及教育同质化等普遍问题作出实质性改善，以便缓解教育的迟钝之处。第二，改变教育。从教育变革的现实需求来看[17]，AIGC可以通过重组教育材料、重设教育环境、重塑教育过程以及重构教育评估，让教育变得更加便捷、精确、灵活与个性化。第三，重塑教育。如果说上述改善与改变还只是意味着教育的智慧进步，那么AIGC推动教育向“智”迭代的关键便在于，它将用一套完全不同于以前的观念系统重塑教育。AIGC的诞生与流行标志着，“即兴的涌现取代了目的论；反身性认知论取代了客观主义；分布式认知取代了自主自律的意志；具身取代了身体被当作心灵和心智的支撑系统；人类和智能机器之间的伙伴关系取代了自由主义主体的昭昭天命”[18]。相应地，AIGC之所以能推动教育的向“智”迭代，一方面是因为教育可以据此重新感知、观察、理解和反思自己的主体（如数据、算法、平台）、性质（如非线性、反身性、互联化）、方法（如知识发现、数据科学、预测分析），进而用与之相匹配的概念设计、理论框架与哲学洞见重新解释教育中教师和学生的角色、知识和课程的本质、方法与技术的基础、教与学的意义。另一方面，则是因为在这样的观念系统中，教育的各种要素不再是简单且相彼此独立的线性关系，而是复杂且能够相互模仿的非线性关系，所以，教育也更有机会藉由数据驱动、知识交叉、智能互联而“涌现”具有放大效应、乘数效应、累积效应的智慧跃迁[19]。</p><p>3</p><p><em><strong>*基于生成式人工智能的教育转型挑战*</strong></em></p><p>基于生成式人工智能的教育转型图景——ChatGPT究竟对教育意味着什么</p><p>从AIGC暗含的弊端来看，围绕它开展的教育转型极有可能面临来自人机冲突、知识魔法、数字鸿沟和超脱现实的挑战。</p><p><strong>(一) 人机冲突</strong></p><p>人机协同一词听起来固然美好，也容易得为人所接受。尤其是，当AIGC与人可以在教育问题上达成共识时，人机协同更具有了事半功倍的效果。然而问题是，AIGC既不是“人”，也可能得出人类理解不了的教育结论，更可能在某些教育问题形成与人不同的意见。此时，人机究竟应该如何协同也就变得不再那么“事有必至，理有固然”。进一步而言，如果教育因为AIGC而接受了人机协同的认知框架，那么人们在作出教育决策时究竟应该听“谁”的，不仅没有不言自明的答案可供参考，反而可能面临以下三类极难解决的人机冲突。（1）AIGC在教育活动中听命于人。如此一来，以下问题便会被凸显。当AIGC作为教学助手出现时，如果教学出现了事故与失败，这个责任应该由“谁”承担；当AIGC用于评价学生发展时，根据概率判断引发歧视等道德问题，它又应该由“谁”来责任；用AIGC极大增强教学效率，但却让现有教师失业，这样的责任如何划分。在此意义上，即便AIGC可以听命于一些人，却也可能因此损害另一些人的利益。（2）AIGC与人在教育活动中共同做主。事实上，无论人还是AIGC拥有了判断教育、规范教育和预测教育的权力，他们都会想法设法将自己的结论“合理化”——ChatGPT饱受批评也正是因为它不会质疑和反对自己。而这种“公说公有理、婆说婆有理”局面，往往只会导致教育的犹豫、软弱甚至混乱，反而搁置了那些原本应该得到解决的问题。（3）在教育活动中人听命于AIGC。这也意味着教育从此变成了一件与人无关的活动。纵然AIGC可以在教育活动中给出更精确、更客观、更科学的答案，但人类也可能从这个“黑箱”中学不到任何东西，甚至可能出现“遇事不决，便问机器”的懒惰与“人假机威”的自大。如此人机协同除了印证AIGC的强大与辉煌，却也让人就此远离了可体悟、可启发、可反思的教育过程。笔者列举上述人机冲突并不是想吓退教育转型的智能热情，而是想表明，如果教育不加批判地接受因为AIGC而看起来很美好的人机协同图景，那么这样的教育转型要么沦为纯粹取悦世人的媚俗观念，要么陷入互为掣肘且无所适从的行动困境。</p><p><strong>(二) 知识魔法</strong></p><p>由图灵所提出的“停机问题”足以表明，由于可计算的东西是有限度的，所以即便所有问题都能用数学模型进行描写，人工智能也解决不了所有问题[20]。这既意味着，由于现阶段的人工智能都无法事先定义教育问题的目的、内容与结果，所以它们只能等着教育问题发生之后才能给出相应的解释。也标志着，无论AIGC的数据多么充分、算法如何高明亦或算力怎样强大，它所生成的知识要么是不相容的，要么是不完备的。其中，不相容意味着，这样的知识既能被证明为真，也能被证明为假；不完美是指这样的知识既无法被证明为真，也无法被证明为假。在这种亦真亦假的可能之间，人们对AIGC的热情非但不会下降，反而可能被某些人撩拨到异乎寻常的高度。一方面，AIGC的背后必然包括那些“取悦于人的算法”[21]，它会倾向于推送各种“人们将要喜欢的东西”，而不是那些“对人很重要的东西”。所以，无论它输出的内容究竟对人意味着什么，最终都会获得如人所愿的美名。另一方面，就像休谟所言，“在探索人类事务时，没有什么事情比确切区分何者为偶然事件、何者为事出有因要求更精确了；也没有什么别的题目比这更容易为某些作者利用，以虚假的妙论和优美的词句自欺欺人”[22]。类似的，那些对AIGC不求甚解之人，难免会曲解它输出的内容以符合自身的直观感受和经验习惯，进而在没有规律之处发现规律、在没有意义之处强加意义、在没有道理之处讲出道理。上述两种趋势相互交替的结果则是，世人将如同魔法欣赏一般对AIGC感到莫名的愉悦和满足，但人们既看不透它到底是突破壁垒和瓶颈的“知识过滤系统”，还是某些人用其贩卖期望和焦虑、攫取名利和荣耀的“知识营销之举”，更无从知晓自己是否会被这些令人愉悦的知识所蒙蔽。显而易见的是，如此知识魔法不仅会令AIGC失去知识突破的意义，也将使得由它输出的内容充满了狡黠之处。所以，人们在肯定AIGC挑战知识分化的同时，寻找既能欣赏其价值、也能检验其事实的教育转型向度就显得颇为重要而又迫切。</p><p><strong>(三) 数字鸿沟</strong></p><p>即便AIGC可以向下兼容弱者、旧技术和老方案以彰显教育转型的公平，但若考虑到这项技术的黑箱性质、数据来源与算法逻辑，它也可能因为凸显数字鸿沟而受到不公平的教育质疑。首先，由黑箱技术引发的教育数字鸿沟。目前可以确定的是，对多数人而言，AIGC依旧意味着黑箱技术。尽管AIGC确实可以解决很多问题，但普通用户对其作用过程却是一无所知，更不用说给予解释。反言之，当教育因为AIGC而出现错误时，一般人也根本无从知晓。更何况，就算他们感受到了这种错误，他们也不知道应该找谁交流，可能连交流什么都未必清楚。这也表明，基于AIGC的教育转型非但无法脱离技术精英的掌控，更难以摆脱“唯技术论”“技术独裁”“技术垄断”的影响，相应地，教育与技术之间的数字鸿沟也将因为AIGC而日益明显。其次，由数据来源引发的教育数字鸿沟。就像ChatGPT应用所呈现的趋势那般，由于它占据了更多英文世界的数据，所以用它进行英文问答也能拥有更好的表现。同理，如果考虑到现实世界中，并非所有人群亦或所有知识（资料）都能产出相同数量和质量的数据，那么，AIGC的生成方向也将朝着那些数据更多且质量更高的人群与知识予以倾斜。具体到教育转型，其结果便是放大某些特定学科、人群、资源与区域的教育优势。再者，由算法逻辑引发的教育数字鸿沟。AIGC的算法逻辑就像其他人工智能产品一样表现为，自适应地为用户提供个性化的内容输出。这也意味着，AIGC将会基于不同人的偏好和需求，以塑造他们接受的教育信息或者知识。这里面固然有其技术进步和人类喜好的考量，但换个角度来看，如此个性化的教育又如何兼顾整体、普遍、共同的教育要求？它又会不会破坏那些有益于精诚团结和增进共识的目标？教育受众会不会习惯性地被自己的兴趣所引导，进而因为AIGC的自适应塑造，导致人与人之间的教育意见越来越分化、激烈和极端？如今的算法逻辑有没有可能让教育转型深陷群体与个体之间的数字鸿沟？列举上述潜在问题既无法、也不是想证明，具有黑箱性质的AIGC就不能为教育所用——专家于普通人而言不也是“黑箱”般的存在，或者不应让那些数据条件更好的人或组织得到优先发展，亦或它一定会让人更自私亦或无法顾及整体要求。而是想提醒大家注意，也许正是因为AIGC太有用且太好用，以至于教育必须尽可能在效能与公平之间作出前瞻性的平衡，以免转身陷入数字鸿沟之中。</p><p><strong>(四) 超脱现实</strong></p><p>ChatGPT最为世人所赞叹的神奇之处，便是它无需像人一样去亲自体验某项活动或者观察与某个问题有关的现象，也能给出有模有样的答案。以至于，人们从它身上看出了另外一种“似人而非人”的智慧。更具体来讲，这种超脱现实的智慧，它既不是来自像“面对面”的沟通交流与实践观察这样的一阶数据（经验系统），也不是诉诸间接推理、归纳、演绎那些与现实相关的二阶数据（理性系统），而是直接依托对数据背后的规律进行三阶数据的挖掘（数位系统）。三阶数据意义上的数位系统既没有现实世界中黑白分明的唯一属性，也不从属于某种抽象的统一架构，所以由它衍生的智慧，既没有传统的本质主义包袱，也难以像过去的智慧一样找到相应的现实形态，故而它指向了某种超脱现实的无序之态[23]。不过，这样的无序并不意味着AIGC就代表着“一团乱麻”或者“误打误撞”的智慧。相反，始于三阶数据的数位系统之所以能发现问题与答案之间的意义关联，其唯一原因是——如同人类的厨房杂乱无序的抽屉有着一个共同点，那就是物品都与食物有关[24]。所以，这样的智慧只会关心“某个问题与什么有关”，而不会在乎“问题的答案是什么”。</p><p>也正是鉴于AIGC超脱现实的智慧本质，它对相关的极致追求既可能让教育中的两个变量如同辛普森悖论一般在分别讨论时都满足了某种性质，可是一旦合并考虑，就可能导致相反的结论；也能使教育中两个本来无甚关联的变量如伯克森悖论一般，体现出貌似强烈的相关关系。如果说这些还只是AIGC因为超脱现实可能带给教育转型的数学谬误，那么以下两种情况则会让这种超脱现实的智慧受到根本性质疑。一方面，如果AIGC仅仅只有十四世纪之前的人类知识及其数据，那么它是不是更有可能成为“地心说”的支持者，而非“日心说”的发现者？毕竟，“地心说”在那个时候更能形成逻辑自洽且为人所接受的回答。同理，仅仅依靠AIGC提供回答的教育转型会不会就是在“地心说”中绕圈圈？另一方面，考虑到AIGC对非线性、反身性、动态性、互联性的“无序”偏好，它显然会从根本上冲击始于标准、排序、分级、分类的教育秩序。以上述两点为鉴，基于AIGC的教育转型终将会变成一件在数位与现实之间寻求平衡、有序与无序之间反复拉扯、在相关与因果之间不断徘徊的挑战性事业。如此一来，即便AIGC开启了全学科、全领域乃至全球范围的科技盛宴，但教育能否据此实现华丽转型却不是波澜不惊、信手拈来的注定之事，反而更像是由太多空白、未知、不确定所形成的精彩冒险。</p><p>4</p><p><em><strong>*基于生成式人工智能的教育转型向度*</strong></em></p><p>基于生成式人工智能的教育转型图景——ChatGPT究竟对教育意味着什么</p><p>正如人们无法将过去500年的教育繁荣及其教训全部归结于印刷机，基于AIGC的教育转型能否实现“化挑战为机遇”的关键仍在于人类对教育的思考及其选择。类似地，如果教育中人可以批判性地检验基于AIGC的教育转型，而不是盲目地接受与欣赏这场关乎效能革命的科技盛宴，那么如此转型也可以被讲述为一段坚持公平的精彩冒险。</p><p><strong>(一) 从接受到批判</strong></p><p>无论是人机协同还是人机冲突，它们在本质上都指向AIGC之于教育既强大也陌生的事实。正因为它足够强大，所以即便它对教育而言是如此的陌生，人机协同也足以构成教育转型的重要选项。然而，教育若是因此便对它倒履相迎，天真地认为接受了人机协同的价值设定与技术要求，教育就会朝着更为美好的方向发展；亦或认为人类总能想到办法把智能机器控制在安全的教育范围，所以人机协同并不是件值得被忧虑的事情；或者认为其他学科总想到解决人机协同麻烦的方案，到时候向其取经便可。那么，基于AIGC的教育转型不仅有可能堕入意料之外的麻烦，还难免让教育陷入手足无措的窘境。更糟的是，从人工智能的未来图景来看，有些技术后果只有在经历数十年的痛苦之后才为人所了解，而到了那个时候，似乎已经没有任何可行的解决方案了，因为要逆转这一进程所带来的破坏过于巨大[25]。所以，教育转型既可能借不到“真经”，也搭不了顺风车。所以，面对既强大且陌生的AIGC，如今之人在接受它成为教育必不可少的一部分之前，有必要像当年那群站在印刷机前的思想家一般[26]，拿起批判的武器在教育转型起点处与之“斗争”：既开放地发掘AIGC之于教育的巨大潜力，同时也不放任那些与之有关的谎言、谬论与混乱。因为，如果AIGC确实代表着教育转型的必然趋势，那么教育中人机协同的真理必然在与人机冲突的谬误的斗争中获得胜利，并因此绽放出更加耀眼的光芒，进而吸引更多人参与到教育转型的事业之中。它意味着，在这个人工智能声望日益增大甚至可能进一步繁荣人类社会的时代背景下，教育转型需要批判性接受的不仅仅是人机协同一般的美好，还包括人机冲突之间的未知、空白和风险；与之同时，教育需要批判的也不止是那些始于直觉、经验、幻想的人类观念，也包括人工智能之于教育的技术霸权、技术偏见和技术垄断；更重要的是，对人机协同进行教育批判，不仅可以唤醒教育中人重新感受、观察、理解人工智能的思想责任，还能为这一概念寻求更为妥帖的教育前提、教育内涵和教育形式，从而为基于AIGC的教育转型注入思想解放的兴趣和信念。</p><p><strong>(二) 从欣赏到检验</strong></p><p>历史地看，无论是计算机还是英特网，它们都曾给教育带来过知识突破的高科技眩晕感，以至于研究者要严肃区分这些技术进步究竟是带来了真实的知识进步，还是循环的陈词滥调或者不知所云的幻想，需要花费越来越多的时间和精力[27]。事到如今，好消息是，AIGC不仅能增大知识的鉴别范围，还能极大缩减知识的鉴别时间；坏消息是，如今的AIGC可以营造出更高级且诡秘的科技眩晕感，从而导致更多人把那些真假难辨的知识当作魔法一般来加以欣赏。有鉴于此，从教育的角度对AIGC进行科学检验、实践检验和交叉检验也就显得尤为重要而又迫切。（1）科学检验。教育将人工智能当作魔法来予以欣赏的根本原因便是无法在科学的尺度上对其给予解释[28]。因此，在教育领域祛除知识魔法的首要任务便是对AIGC进行科学检验，即对AIGC的数据、算法和受众作出科学的教育解释。内容包括，数据的样本设计、选择标准与结果呈现；算法的逻辑结构、模型参数、主要用途；教育受众对AIGC的接受程度与理解程度。（2）实践检验。现阶段的AIGC并不会否定自己。不过，人类却是因为可以意识到自己不知道什么，从而通过否定自己的方式来实现知识跃迁[29]。更何况，任何始于技术进步的教育转型，它们在开始阶段既难免存在知识瑕疵，也可能面临诸多难以解释的“意外”和“不合理”。但这不应成为否定教育转型的借口。相反，这些“意外”“不合理”本身可能蕴含着教育转型的新范式。因此，从教育的角度对AIGC进行实践检验就是要在欣赏其成就的同时，实事求是地发现并揭露它在知识输出上的不确定、不一致和不相容，以便为迎接教育新范式做好准备。（3）交叉检验。无论是人还是AIGC，它们都会倾向于逻辑一致地将事物（问题）合理化，所以，能否拆穿上述知识魔法，就需要通过跨领域的交叉检验，以打破基于教育或者AIGC的“自说自话”。这意味着，无论是藉由AIGC得出的教育知识，还是教育自身生产的知识，亦或教育从其他领域、其他学科、其他组织“迁移”过来的知识，它们必定能够在相同的逻辑基础上接近共同事实、得出类似答案以及揭示相近规律。也因为跨领域的交叉检验作为保障，教育中人不仅更有把握确保自己不会因为对AIGC的欣赏而蒙蔽了双眼，也更有可能看清那些在教育实践中有原创、有超越、有新意的知识突破，更将让那些似是而非的回答、超出理解范围的解释、不切实际的描述以及由此形成的知识魔法得到应有的祛魅。</p><p><strong>(三) 从效能到公平</strong></p><p>ChatGPT已经用自己的成功证明了AIGC的巨大效能。而这还仅仅只是开始。毕竟从人工智能的指数规律来看，它的能力和效率正在成倍增加，而成本却在下降[30]。在此意义上，AIGC无疑会凭借自身的巨大效能进一步加快教育转型的脚步。然而，就像前文所表明的那般，如果放任AIGC加大教育群体的数字鸿沟，那么，在它极大增强教育效能的同时，会不会恶化教育的马太效应？一方面，教育和技术的竞赛已经表明，尽管新技术无法预言谁一定会成为教育中的优胜者，但那些教育基础雄厚、拥有更多资源、离新技术更近的学校、学科或学生更有可能先人一步攫取技术优势[31]。以此类推，新技术更有可能强化教育领域固有的强弱格局，乃至加剧教育的不公平。另一方面，若是考虑到AIGC强调主动、创新、提问的观念与教育中弱势群体被动、保守、沉默的思想之间的巨大对立，那么这种新技术在弱势群体得到良好应用的概率显然要低于强势群体。因此之故，面对基于AIGC的教育转型，人们除了赞叹其效能，更有必要强调其公平。不仅是因为与教育中强势学校、强势学生和强势学科相比，那些弱势学校、弱势学生和弱势学科可能对此所知更少甚至毫无准备；也是因为即便AIGC拥有巨大效能，但它本身并不能决定、也不在乎由“谁”来使用或者用来做什么。更重要的理由在于，与因为弱势群体缺乏条件或者用不好就拒绝他们共享AIGC的巨大效能相比，采取以下举措更有可能塑造教育的新格局。其一，在AIGC真正进入教育之前，提供面向弱势群体的专向建设和专项指导，改善他们在受教育过程中接触利用数字工具的条件，转变他们的教育观念，提升他们的数字知识、数字技能与数字信心，增加教育弱势群体参与教育转型的数字素养与数字成熟度，以便更多人有机会参与其中。其二，在围绕AIGC设计教育活动的时，可以立足于让不同群体在获取（分配）数据、信息以及知识时能够受到一视同仁的对待；可以不再迫使每个人遵循相同且僵化的标准化系统，而是立足于让每个人成为更好的自己；可以反应个体的能力、基础和天赋，而不是屈从于固定的背景变量（如成绩、区域、身份）；可以为赋能个人学习和发展而进行设计，而不是拘泥于所谓平均水平与标准模型，以便更多人有热情参与其中。立足于让更多人从AIGC中受益，如此教育转型才能超越与人无关的“技术神话”和于人无益的“技术负担”，日益固化的教育格局方才有可能被重塑。</p><p><strong>(四) 从盛宴到冒险</strong></p><p>由ChatGPT掀起的科技盛宴仍在继续，甚至可能因为GPT4.0模型和GPT5.0模型呈现出愈演愈烈之势。不过令人忧虑之处在于，AIGC要是真能轻而易举地改变教育，那么“上个世纪60年代就已经被发明且得到认可的智能教学系统又何至于到了今天仍然无法推动教育实现真正意义上的进步”[32]。从过去到现在都被视作改变教育重要力量的互联网，又何至于“沦为谣言、流言与谎言的集合，人类注意力的粉碎者，人类长线思考的终结者，代表了粗鄙者的崛起、剽窃者的胜利、文化的终结和一个黑暗时代的开始”[33]。诸如此类事实可以表明，不是所有技术意义上可以实现的问题解决方案都能顺利改变教育；而且，基于技术进步的教育除了可以解决旧问题，自身也会带来新问题；所以，基于AIGC的教育转型很可能无法直接导向更美好的教育未来。换言之，AIGC在向教育发出参与科技盛宴的邀请函的同时，也向它发出了参与冒险的挑战书。因为在这场由AIGC扮演主角的宴会中，不仅存在着完全不同于教育的手段属性与超脱现实的本质规律，更包括诸多相伴而生的教育之谜与教育风险。</p><p>在此局面下，教育中人能否像“冒险家”一样出席这场科技盛宴，不仅决定了教育能否实现向“智”迭代，也决定了其他人能否在AIGC浪潮中听到教育的声音，更将决定这场盛宴的精彩程度。一方面是因为，AIGC的发展需要诉诸教育的形式以获得更多高质量人才。不过，考虑到AIGC掌握常规知识的无比能力，这样的高质量人才培养显然不能再依托那些屡见不鲜甚至周而复始的知识及其传授方法。故而，教育中人必须像“冒险家”一般，勇敢地走出整齐划一、安全熟悉、非此即彼的知识范围，转而凭借教育的人文优势以获得徘徊于“数位与现实”“有序与无序”“相关与因果”之间的知识洞见。比如，（1）在应对不确定性、模糊性和复杂性上的独特贡献；（2）对多元差异和非主流观点的维护；（3）对各种矛盾（冲突）的协调；（4）为那些无法预见可能性的事物和观念保留余地；（5）将种种细节（数据）被整合为有意义的现实图景。另一方面则是因为，只要AIGC在思想与道德上引发的问题有可能高于它所解决的问题，那么它的相关研究与实践就必须将教育视作目的，以便从根本上缓解、应对和预防有关问题给人类社会带来的伤害。毕竟，这些源自思想与道德的问题隶属于教育的传统事务，而非科学命题或者技术难题。概言之，当AIGC因为教育而拥有更加安全、稳健、可持续的未来之时，这场科技盛宴也将绽放出更强的生命力和影响力。</p><p> 参考文献：</p><p>[1] [美]伊丽莎白·爱森斯坦.何道宽译.作为变革动因的印刷机——早期近代欧洲的传播与文化变革[M].北京：北京大学出版社，2010.</p><p>[2] [意]卢西亚诺·弗洛里迪.王文革译.第四次革命：人工智能如何重塑人类现实[M].杭州：浙江人民出版社，2016.</p><p>[3] 杨欣.教育数字化转型的算法机遇、挑战与调适[J].高等教育研究，2022，（2）：13-22.</p><p>[4]王雅静.乔姆斯基：ChatGPT于教育无益？[EB&#x2F;OL].https：&#x2F;&#x2F;<a href="http://www.sohu.com/a/651148238_100189681%EF%BC%8C2023-03-08">www.sohu.com/a/651148238_100189681，2023-03-08</a>.</p><p>[5][28]杨欣.魔法与科学：人工智能的教育迷思及其祛魅[J].教育学报，2021，（2）：18-31.[6][英]齐格蒙特·鲍曼.邵迎生译.现代化与矛盾性[M].北京：现代性与矛盾性，2013.</p><p>[7][16][英]安东尼.塞尔登，[英]奥拉迪梅吉.阿比多耶.吕晓志译.第四次教育革命：人工智能如何改变教育[M].北京：机械工业出版社，2019.</p><p>[8][25][美]蒂姆·奥莱利.杨晨曦，戴茗玥等译.未来地图：技术、商业和我们的选择[M].北京：电子工业出版社，2018.</p><p>[9][20][美]爱德华·阿什福德·李.张凯龙，冯红译.柏拉图与技术呆子：人类与技术的创造性伙伴关系[M].北京：中信出版社，2020.</p><p>[10]杨欣.教育评价改革的算法追问[J].华东师范大学学报（教育科学版），2022，（1）：19-29.</p><p>[11][33][美]戴维.温伯格.胡泳，高美译.知识的边界[M].太原：山西出版社，2015.</p><p>[12]杨欣.智能时代教育异化的表征、病灶及治理[J].中国电化教育，2021，（8）：34-41.</p><p>[13][18][美]凯瑟琳·海勒.刘宇清译.我们如何成为后人类：文学、信息科学和控制论中的虚拟身体[M].北京：北京大学出版社，2017.</p><p>[14]杨欣.人工智能“智化”教育的内涵、途径和策略——人工智能何以让教育变得更聪明[J].中国电化教育，2020，（3）：25-31.</p><p>[15][美]佩德罗·多明戈斯.黄芳萍译.终极算法——机器学习和人工智能如何重塑世界[M].北京：中信出版社，2017.</p><p>[17]杨欣.人工智能与教育深度融合的创新、挑战及路向——如何讲好智能时代的教育故事[J].开放教育研究，2021，（3）：37-45.</p><p>[19]杨欣.人工智能立场中的智慧教育：理据、内涵与特征[J].现代教育技术，2021，（4）：5-12.</p><p>[21][立]伊格纳斯·卡尔波卡斯.邱遥堃译.算法治理：后人类时代的政治与法律[M].上海：上海人民出版社，2022.</p><p>[22][英]休谟.张若衡译.休谟政治论文选[M].北京：商务印书馆，2018.</p><p>[23][24][美]戴维·温伯格.李燕鸣译.万物皆无序[M].太原：山西人民出版社，2017.</p><p>[26][英]汤姆·斯丹迪奇.林华译.社交媒体简史[M].北京：中信出版社，2015.</p><p>[27][英]彼得·帕克.汪一帆，赵博囡译.知识社会史·下卷：从《百科全书》到维基百科[M].杭州：浙江大学出版社，2016.</p><p>[29][以]尤瓦尔.赫拉利.林俊宏译.人类简史：从动物到上帝[M].北京：中信出版社，2017.</p><p>[30][加]彼得·戴曼迪斯，[加]史蒂芬·科特勒.贾拥民译.未来呼啸而来[M].北京：北京联合出版公司，2021.</p><p>[31][美]克劳迪娅·戈尔丁，[美]劳伦斯·凯兹.陈津竹，徐黎蕾译.教育和技术的竞赛[M].北京：商务印书馆，2015.</p><p>[32][美]斯图尔特·罗素.张羿译.AI新生：破解人机共存密码——人类最后一个大问题[M].北京：中信出版社，2020.</p><p>作者简介：</p><p>杨欣，副教授，博士，硕士生导师，研究方向为教育数字化转型、基础教育质量监测与评价。</p><p>∗本文系西藏自治区教育科学研究2020年度重大课题“教育信息化促进西藏基础教育改革发展的实践探索与研究”（课题编号：XZJYKTZD03）、中国基础教育质量监测协同创新中心国家监测专项任务“教师一般育人能力研究”（项目编号：202206032BZPK01）研究成果。</p><p>*本文仅用于教学，如果分享内容侵犯您的版权或者非授权发布，请及时与我们联系，我们会及时删除。</p>]]></content>
    
    
    
    <tags>
      
      <tag>ChatGPT</tag>
      
      <tag>生成式人工智能</tag>
      
      <tag>教育转型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>通向AGI之路：大型语言模型（LLM）技术精要</title>
    <link href="/2023/06/06/3/"/>
    <url>/2023/06/06/3/</url>
    
    <content type="html"><![CDATA[<p>本文来源：<a href="https://zhuanlan.zhihu.com/p/597586623">https://zhuanlan.zhihu.com/p/597586623</a></p><p>ChatGPT出现后惊喜或惊醒了很多人。惊喜是因为没想到大型语言模型（LLM,Large Language Model）效果能好成这样；惊醒是顿悟到我们对LLM的认知及发展理念，距离世界最先进的想法，差得有点远。我属于既惊喜又惊醒的那一批，也是典型的中国人，中国人善于自我反思，于是开始反思，而这篇文章正是反思的结果。</p><p>实话实说，国内在LLM模型相关技术方面，此刻，距离最先进技术的差距进一步加大了。技术领先或技术差距这事情，我觉得要动态地以发展的眼光来看。在Bert出现之后的一到两年间，其实国内在这块的技术追赶速度还是很快的，也提出了一些很好的改进模型，差距拉开的分水岭应该是在 GPT 3.0出来之后，也就是2020年年中左右。在当时，其实只有很少的人觉察到：GPT 3.0它不仅仅是一项具体的技术，其实体现的是LLM应该往何处去的一个发展理念。自此之后，差距拉得越来越远，ChatGPT只是这种发展理念差异的一个自然结果。所以，我个人认为，抛开是否有财力做超大型LLM这个因素，如果单从技术角度看，差距主要来自于对LLM的认知以及未来应往何处去的发展理念的不同。</p><p>国内被国外技术甩得越来越远，这个是事实，不承认也不行。前阵子网上很多人担忧说国内AI现在处于“危急存亡之秋”，我觉得倒也不至于这么严重。君不见，这个世界上，具备这么超前眼光的只有OpenAI一家吗？包括Google在内，其实对于LLM发展理念的理解，明显都落后OpenAI一个身位。现实是OpenAI表现过于优秀，把所有人都甩开了，不仅仅是国内。</p><p>我觉得，OpenAI对LLM在理念及相关技术方面，领先国外的Google、DeepMind大约半年到一年的时间，领先国内大概两年左右的时间。在LLM这个事情上，感觉梯队很明显，Google应该是排在第二位，最能体现Google技术眼光的是PaLM和Pathways，推出时间大概在22年2月到4月间，同一时期，OpenAI推出的却是InstructGPT，从这里就可以看出Google和OpenAI的差距了，至于为何这么说，你看了我后面的正文后大概能理解。DeepMind之前的重心一直在强化学习攻克游戏和AI for science这些方面，切入LLM其实很晚，应该是21年才开始重视这个方向，目前也处于追赶状态。Meta就更不用说了，重心一直不在LLM上，目前感觉也发力开始追赶。这还是目前做得最好的一批机构，尚且如此，更何况国内呢？我觉得情有可原。至于OpenAI关于LLM的理念是什么，我在本文的最后一部分，会谈谈我的认知。</p><p>本文梳理自GPT 3.0出现之后的主流LLM技术，在此之前的主流技术可以参考：</p><p><a href="https://zhuanlan.zhihu.com/p/254821426">乘风破浪的PTM：两年来预训练模型的技术进展1562 赞同 · 60 评论文章<img src="https://pic1.zhimg.com/v2-0c070d4ef5c5122d7b6075010518ec92_r.jpg?source=172ae18b"></a></p><p>，我相信看完这两篇文章，能够让您对LLM领域的技术脉络，LLM技术发展过程中出现过的不同发展理念，乃至未来可能的发展趋势，有比较清晰的认知。当然，很多地方讲的内容是我个人看法，有很大的主观性，错漏难免，所以还请谨慎参考。</p><p>本文试图回答下面一些问题：ChatGPT是否带来了NLP乃至AI领域的研究范式转换？如果是，那会带来怎样的影响？LLM从海量数据中学到了什么知识？LLM又是如何存取这些知识的？随着LLM规模逐步增大，会带来什么影响？什么是In Context Learning?为什么它是一项很神秘的技术？它和Instruct又是什么关系？LLM具备推理能力吗？思维链CoT又是怎么做的？等等，相信看完，能让您对这些问题有一个答案。</p><p>首先，在谈LLM技术现状前，先宏观地谈下我心目中的研究范式转换问题。这样，我们才能“先见森林，再见树木”，对具体技术为何会是如此变化有个更清晰的认知。</p><h2 id="潮流之巅：NLP研究范式的转换"><a href="#潮流之巅：NLP研究范式的转换" class="headerlink" title="潮流之巅：NLP研究范式的转换"></a>潮流之巅：NLP研究范式的转换</h2><p>如果我们把时间线往前拉得更长一些，回到NLP领域的深度学习时代，在更长时间窗口内观察技术变迁及其影响，可能会更容易看清其中的一些关键节点。我个人认为，在最近10年来NLP领域的技术发展过程中，可能存在两次大的研究范型转换。</p><h3 id="范式转换1-0-从深度学习到两阶段预训练模型"><a href="#范式转换1-0-从深度学习到两阶段预训练模型" class="headerlink" title="范式转换1.0:从深度学习到两阶段预训练模型"></a>范式转换1.0:从深度学习到两阶段预训练模型</h3><p>这个范式转换所涵盖的时间范围，大致在深度学习引入NLP领域（2013年左右），到GPT 3.0出现之前（2020年5月左右）。</p><p>在Bert和GPT模型出现之前，NLP领域流行的技术是深度学习模型，而NLP领域的深度学习，主要依托于以下几项关键技术：以大量的改进LSTM模型及少量的改进CNN模型作为典型的特征抽取器；以Sequence to Sequence（或叫encoder-decoder亦可）+Attention作为各种具体任务典型的总体技术框架。</p><p>在这些核心技术加持下，NLP领域深度学习的主要研究目标，如果归纳一下，是如何有效增加模型层深或模型参数容量。就是说，怎么才能往encoder和decoder里不断叠加更深的LSTM或CNN层，来达成增加层深和模型容量的目标。这种努力，尽管确实不断增加了模型层深，但是从解决具体任务的效果角度看，总体而言，不算很成功，或者说和非深度学习方法相对，带来的优势不算大。</p><p>深度学习之所以不够成功，我认为主要原因来自于两个方面：一方面是某个具体任务有限的训练数据总量。随着模型容量的增加，需要靠更大量的训练数据来支撑，否则即使你能把深度做起来，任务效果也做不上去。而在预训练模型出现之前，很明显这是NLP研究领域一个严重问题；另外一个方面是LSTM／CNN特征抽取器，表达能力不够强。意思是就算给你再多的数据也没用，因为你不能有效地吸收数据里蕴含的知识。主要应该是这两个原因，阻碍了深度学习在NLP领域的成功突围。</p><p>Bert&#x2F;GPT这两个预训练模型的出现，无论在学术研究角度看，还是工业应用角度来看，都代表了NLP领域的一个技术飞跃，并带来了整个领域研究范式的转换。这种范式转换带来的影响，体现在两个方面：首先，是部分NLP研究子领域的衰退乃至逐步消亡；其次，NLP不同子领域的技术方法和技术框架日趋统一，在Bert出现后一年左右，技术栈基本收敛到两种技术模式中。关于这两点，我们分头来谈。</p><p><strong>影响一：中间任务的消亡</strong></p><p>NLP是一个宏观研究领域的统称，里面有五花八门具体的子领域与子方向，如果仔细分析，从任务的性质角度，可以把这些任务分成两大类：一类可以叫做“中间任务”，一类可以称为“最终任务”。</p><p>典型的中间任务包括：中文分词、词性标注、NER、句法分析、指代消解、语义Parser等，这类任务一般并不解决应用中的实际需求，大多数是作为那些解决实际需求任务的中间阶段或者辅助阶段存在的，比如几乎没有需求说，我要一个句法Parser，把这个句子的句法分析树给用户看看，用户不需要看到这些NLP的中间阶段处理结果，他只关心某个具体任务你有没有干好。“最终任务”包括比如文本分类、文本相似性计算、机器翻译、文本摘要等等，有很多。这类任务的特点是每个子领域都解决某个实际需求，任务结果基本能直接呈现给用户，比如用户确实存在给你一句英文，告诉他中文是什么的需求。</p><p>按理说，“中间任务”就不应该出现，而之所以会存在，这是NLP技术发展水平不够高的一种体现。在技术发展早期阶段，因为当时的技术相对落后，很难一步做好有难度的最终任务。比如机器翻译，早期技术要做好机器翻译是很困难的，于是科研人员就把难题分而治之，分解成分词、词性标注、句法分析等各种中间阶段，先把每个中间阶段做好，然后再拼起来完成最终任务，这也是没办法的事情。</p><p>但是自从Bert／GPT出现之后，其实就没有必要做这些中间任务了，因为通过大量数据的预训练，Bert／GPT已经把这些中间任务作为语言学特征，吸收到了Transformer的参数里，此时我们完全可以端到端地直接解决那些最终任务，而无须对这种中间过程专门建模。这里可能争议最大的是中文分词，其实道理也是一样的，哪些字应该组成一个词，这个其实你不用管，让LLM自己当特征去学就行了，只要对于解决任务有帮助，它自然会去学该学的合理分词方式，也未必一定要和我们人类理解的分词规则相同。</p><p>基于以上认知，其实在Bert&#x2F;GPT一出现，你就应该得出这类NLP的中间阶段的任务，会逐步退出历史舞台这个结论。</p><p><strong>影响二：不同研究方向技术路线的统一</strong></p><p>在说明具体影响前，我们先讨论下另外一种NLP任务划分方式，这对于理解后面内容有帮助。如果对“最终任务”进一步进行分类，又大致可以分为两大不同类型的任务：自然语言理解类任务和自然语言生成类任务。如果排除掉“中间任务”的话，典型的自然语言理解类任务包括文本分类、句子关系判断、情感倾向判断等，这种任务本质上都是分类任务，就是说输入一个句子（文章），或者两个句子，模型参考所有输入内容，最后给出属于哪个类别的判断。自然语言生成也包含很多NLP研究子方向，比如聊天机器人、机器翻译、文本摘要、问答系统等。生成类任务的特点是给定输入文本，对应地，模型要生成一串输出文本。这两者的差异主要体现在输入输出形式上</p><p>自从Bert&#x2F;GPT模型诞生后，出现了明显的技术统一趋向。首先，NLP中不同的子领域，其特征抽取器都逐渐从LSTM&#x2F;CNN统一到Transformer上。其实，自Bert公开后不久，就应该意识到，这必然会成为技术趋势。至于其原因，在几年前我写的这篇：“</p><p><a href="https://zhuanlan.zhihu.com/p/54743941">放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN&#x2F;RNN&#x2F;TF）比较3374 赞同 · 315 评论文章<img src="https://pic1.zhimg.com/v2-427d3a804d490704ac70fb8d5c0a7fe7_r.jpg?source=172ae18b" alt="img"></a></p><p>” 中做了说明和分析，感兴趣的同学可参考。而且，目前Transformer不仅统一了NLP诸多领域，也正在逐步地替换图像处理各种任务中被广泛使用的CNN等其它模型的进程之中，类似的，多模态模型目前也基本都采用了Transformer模型。这种Transformer从NLP出发，攻城略地逐步统一AI越来越多领域的趋势，起始于2020年底出现的Vision Transformer (ViT) ，之后蓬勃发展，到目前已大获成功，且其继续向更多领域拓展的势头会越来越迅猛。</p><p>其次，大多数NLP子领域的研发模式切换到了两阶段模式：模型预训练阶段+应用微调（Fine-tuning）或应用Zero／Few Shot Prompt模式。更准确地说，NLP各种任务其实收敛到了两个不同的预训练模型框架里：对于自然语言理解类任务，其技术体系统一到了以Bert为代表的“双向语言模型预训练+应用Fine-tuning”模式；而对于自然语言生成类任务，其技术体系则统一到了以GPT 2.0为代表的“自回归语言模型（即从左到右单向语言模型）+Zero &#x2F;Few Shot Prompt”模式。至于为何会分化成两条技术路线，有其必然性，关于这点我们放在后面解释。</p><p>这两种模式，看似比较相像，但其背后蕴含了迥异的发展思路，也会导向不同的未来发展方向。不过遗憾的是，我们中的绝大多数人，在当时都低估了GPT 这条发展路线的潜力，而把视觉中心聚焦到了Bert这种模式上。</p><h3 id="范式转换2-0-从预训练模型走向通用人工智能-（AGI，Artificial-General-Intelligence）"><a href="#范式转换2-0-从预训练模型走向通用人工智能-（AGI，Artificial-General-Intelligence）" class="headerlink" title="范式转换2.0: 从预训练模型走向通用人工智能 （AGI，Artificial General Intelligence）"></a><strong>范式转换2.0: 从预训练模型走向通用人工智能 （AGI，Artificial General Intelligence）</strong></h3><p>这个范式转换所涵盖的时间范围，大致在GPT3.0出现之后（20年6月左右），一直到目前为止，我们应该正处于这个范式转换过程中。</p><p>ChatGPT是触发这次范型转换的关键节点，但是在InstructGPT出现之前，其实LLM处于这次范式转换前的一个过渡期。</p><p><strong>过渡期：以GPT 3.0为代表的“自回归语言模型+Prompting”模式占据统治地位</strong></p><p>前面说过，在预训练模型发展的早期，技术框架收敛到了Bert模式和GPT模式这两种不同的技术范型，而且人们普遍更看好Bert模式一些，相当多数的后续技术改进，都是沿着Bert那条路走的。但是，随着技术的继续发展，你会发现，目前规模最大的LLM模型，几乎清一色都是类似GPT 3.0这种“自回归语言模型+Prompting”模式的，比如GPT 3、PaLM、GLaM、Gopher、Chinchilla、MT-NLG、LaMDA等，没有例外。为什么会这样呢？背后一定有其必然性，我认为可能主要源于两个原因。</p><p><img src="https://pic2.zhimg.com/80/v2-f0276d09a56109bce54cefcef033fed9_1440w.webp" alt="Google的T5模型"></p><p>首先，Google的T5模型，在形式上统一了自然语言理解和自然语言生成任务的外在表现形式。如上图所示，标为红色的是个文本分类问题，黄色的是判断句子相似性的回归或分类问题，这都是典型的自然语言理解问题。在T5模型里，这些自然语言理解问题在输入输出形式上和生成问题保持了一致，也就是说，可以把分类问题转换成让LLM模型生成对应类别的字符串，这样理解和生成任务在表现形式就实现了完全的统一。</p><p>这说明自然语言生成任务，在表现形式上可以兼容自然语言理解任务，若反过来，则很难做到这一点。这样的好处是：同一个LLM生成模型，可以解决几乎所有NLP问题。而如果仍然采取Bert模式，则这个LLM模型无法很好处理生成任务。既然这样，我们当然倾向于使用生成模型，这是一个原因。</p><p>第二个原因，如果想要以零示例提示语（zero shot prompting）或少数示例提示语（few shot prompting）的方式做好任务，则必须要采取GPT模式。现在已有研究（参考：On the Role of Bidirectionality in Language Model Pre-Training）证明：如果是以fine-tuning方式解决下游任务，Bert模式的效果优于GPT模式；若是以zero shot&#x2F;few shot prompting这种模式解决下游任务，则GPT模式效果要优于Bert模式。这说明了，生成模型更容易做好zero shot&#x2F;few shot prompting方式的任务，而Bert模式以这种方式做任务，是天然有劣势的。这是第二个原因。</p><p>但是问题来了：为什么我们要追求zero shot&#x2F;few shot prompting这种方式来做任务呢？要解释清楚这个问题，我们首先需要搞清楚另外一个问题：什么样的LLM模型，对我们是最理想的？</p><p><img src="https://pic1.zhimg.com/80/v2-11caaebf977aae428fd8a6302dea2e60_1440w.webp" alt="大语言模型架构"></p><p>上图展示了一个理想的LLM该有的样子。首先，LLM应该具备强大的自主学习能力。假设我们把世界上能获得的所有文本或者图片等不同类型的数据喂给它，它应该能够自动从中学习到里面包含的所有知识点，学习过程不需要人的介入，并且能灵活应用所学知识，来解决实际问题。因为数据是海量的，要吸收所有知识，就要非常多的模型参数来存储知识，所以这个模型必然会是一个巨无霸模型。</p><p>其次，LLM应该能解决NLP任何子领域的问题，而不仅支持有限领域，甚至它应该可以响应NLP之外其它领域的问题，最好是任意领域的问题都能得到很好地回答。</p><p>再者，当我们使用LLM解决某个具体领域问题的时候，应该用我们人类习惯的表达方式，就是说LLM应该理解人类的命令。这体现出让LLM适配人，而不是反过来，让人去适配LLM模型。人适配LLM的典型例子，比如绞尽脑汁去尝试各种不同的prompt，以试图找到好的提示语，才能很好地解决手头问题。关于这点，上图在人类和LLM交互的接口层，举了几个例子，说明什么是好的人使用LLM模型的接口形式。</p><p>看完这个理想中的LLM，我们再回头解释上面遗留的问题：为什么我们要追求zero shot&#x2F;few shot prompting这种方式来做任务呢？有两个原因。</p><p>第一，这个LLM模型规模必然非常巨大，有能力作出这个模型，或改动这个模型参数的机构必然很少。而任务需求方是千千万万的中小机构甚至是个人，就算你把模型开源出来，他们也无力部署这个模型，更不用说再用Fine-tuning这种模式去修改模型参数了。所以，我们应该追求不修正模型参数，就能让任务需求方完成任务的方式，也就是应该采取prompt模式完成任务，而非Fine-tuning模式（由此可看出，soft prompting技术方向是违背这个发展趋势的）。模型制作方则将LLM作成公用服务，以LLM as Service的模式运行。作为服务支持方，考虑到千变万化的用户需求，所以LLM模型制作方更要追求让LLM能完成尽可能多类型的任务，这是附带的影响，也是为何超级大模型一定会追求走向AGI的现实因素。</p><p>第二，zero shot prompting也好，few shot prompting也好，甚至促进LLM推理能力的思维链（CoT,Chain of Thought）Prompting也好，就是上图中接口层中的现有技术。具体而言，zero shot prompting的初衷，其实就是人类和LLM的理想接口，直接用人类所习惯的任务表述方式让LLM做事情，但是发现LLM并不能很好地理解，效果也不好。经过继续研究，转而发现：对于某项任务，如果给LLM几个示例，用这些示例来代表任务描述，效果会比zero shot prompting好，于是大家都去研究更好的few shot prompting技术。可以理解为，本来我们希望LLM能够用人类常用的命令方式来执行某个任务，但是目前技术还做不到，所以退而求其次，用这些替代技术来表达人类的任务需求。</p><p>如果理解了上述逻辑，很容易得出如下结论：few shot prompting（也被称为In Context Learning）只是一种过渡时期的技术。如果我们能够更自然地去描述一个任务，而且LLM可以理解，那么，我们肯定会毫不犹豫地抛弃这些过渡期的技术，原因很明显，用这些方法来描述任务需求，并不符合人类的使用习惯。</p><p>这也是为何我将GPT 3.0+Prompting列为过渡期技术的原因，ChatGPT的出现，改变了这个现状，用Instruct取代了Prompting，由此带来新的技术范式转换，并产生若干后续影响。</p><p><strong>影响一：让LLM适配人的新型交互接口</strong></p><p>在理想LLM的背景下，我们再来看ChatGPT，能更好理解它的技术贡献。ChatGPT应该是目前所有的现有技术里，最接近理想LLM的技术方法。如果归纳下ChatGPT最突出特点的话，我会用下面八个字：“能力强大，善解人意”。</p><p>“能力强大”这一点，我相信应该主要归功于ChatGPT所依托的基础LLM GPT3.5。因为ChatGPT 尽管加入了人工标注数据，但是量级只有数万，这个规模的数据量，和训练GPT 3.5模型使用的几千亿token级别的数据量相比，包含的世界知识（数据中包含的事实与常识）可谓沧海一粟，几可忽略，基本不会对增强GPT 3.5的基础能力发挥什么作用。所以它的强大功能，应该主要来自于隐藏在背后的GPT 3.5。GPT 3.5对标理想LLM模型中的那个巨无霸模型。</p><p>那么，ChatGPT向GPT 3.5模型注入新知识了吗？应该是注入了，这些知识就包含在几万人工标注数据里，不过注入的不是世界知识，而是人类偏好知识。所谓“人类偏好”，包含几方面的含义：首先，是人类表达一个任务的习惯说法。比如，人习惯说：“把下面句子从中文翻译成英文”，以此表达一个“机器翻译”的需求，但是LLM又不是人，它怎么会理解这句话到底是什么意思呢？你得想办法让LLM理解这句命令的含义，并正确执行。所以，ChatGPT通过人工标注数据，向GPT 3.5注入了这类知识，方便LLM理解人的命令，这是它“善解人意”的关键。其次，对于什么是好的回答，什么是不好的回答，人类有自己的标准，例如比较详细的回答是好的，带有歧视内容的回答是不好的，诸如此类。这是人类自身对回答质量好坏的偏好。人通过Reward Model反馈给LLM的数据里，包含这类信息。总体而言，ChatGPT把人类偏好知识注入GPT 3.5，以此来获得一个听得懂人话、也比较礼貌的LLM。</p><p>可以看出，ChatGPT的最大贡献在于：基本实现了理想LLM的接口层，让LLM适配人的习惯命令表达方式，而不是反过来让人去适配LLM，绞尽脑汁地想出一个能Work的命令（这就是instruct技术出来之前，prompt技术在做的事情），而这增加了LLM的易用性和用户体验。是InstructGPT&#x2F;ChatGPT首先意识到这个问题，并给出了很好的解决方案，这也是它最大的技术贡献。相对之前的few shot prompting，它是一种更符合人类表达习惯的人和LLM进行交互的人机接口技术。</p><p>而这必将启发后续的LLM模型，继续在易用人机接口方面做进一步的工作，让LLM更听话。</p><p><strong>影响二：很多NLP子领域不再具备独立研究价值</strong></p><p>就NLP领域而言，这次范式转换，意味着很多目前独立存在的NLP研究领域，将被纳入LLM的技术体系，进而不再独立存在，逐步消失。经过第一次范式转换，尽管NLP中很多“中间任务”，继续作为独立研究领域存在不再必要，但是大多数“最终任务”，仍然是以独立研究领域存在的，只是切换成在“预训练+fine-tuning”框架下，面对领域独有问题，陆续提出新的改进方案。</p><p>目前研究表明，很多NLP任务，随着LLM模型规模增长，效果会大幅提升。据此，我觉得可得到如下推论：大多数某领域所谓“独有”的问题，大概率只是缺乏领域知识导致的一种外在表象，只要领域知识足够多，这个所谓领域独有的问题，就可以被很好地解决掉，其实并不需要专门针对某个具体领域问题，冥思苦想去提出专用解决方案。也许AGI的真相超乎意料地简单：你只要把这个领域更多的数据交给LLM，让它自己学习更多知识即可。</p><p>在这个背景下，同时，ChatGPT证明了我们现在是可以直接去追求理想LLM模型的，那么，未来的技术发展趋势应该是：追求规模越来越大的LLM模型，通过增加预训练数据的多样性，来涵盖越来越多的领域，LLM自主从领域数据中通过预训练过程学习领域知识，随着模型规模不断增大，很多问题随之得到解决。研究重心会投入到如何构建这个理想LLM模型，而非去解决某个领域的具体问题。这样，越来越多NLP的子领域会被纳入LLM的技术体系，进而逐步消失。</p><p>我认为，判断某个具体领域是否该立即停止独立研究，其判断标准可采取以下两种方法，占其一即可：第一，判断某个任务，是否LLM的研究效果超过人类表现，对于那些LLM效果超过人类的研究领域，已无独立研究的必要。举个例子，GLUE与SuperGLUE测试集合里的很多任务，目前LLM效果已超过人类表现，与这个数据集合密切相关的研究领域，其实就没有继续独立存在的必要。第二，对比两种模式的任务效果，第一种模式是用较大的领域专用数据进行Fine-tuning，第二种是few-shot prompting或instruct-based方法。如果第二种方法效果达到或超过第一种方法，则意味着这个领域没有继续独立存在的必要性。如果用这个标准来看，其实很多研究领域，目前fine-tuning效果还是占优的（因为这种模式领域训练数据量大），看似还可独立存在。但是考虑到很多任务随着模型规模增大，few shot prompting效果持续增长，随着更大模型的出现，这个拐点很可能短期就会达到。</p><p>如果上述猜测成立，将意味着如下残酷事实：对于很多NLP领域的研究人员，将面临往何处去的选择，是继续做领域独有问题呢？还是放弃这种看似前途不大的方式，转而去建设更好的LLM？如果选择转向去建设LLM，又有哪些机构有能力、有条件去做这个事情呢？你对这个问题的回答会是什么呢？</p><p><strong>影响三：更多NLP之外的研究领域将被纳入LLM技术体系</strong></p><p>如果站在AGI的视角，参照之前描述的理想LLM模型，它所能完成的任务，不应局限于NLP领域，或某一两个学科领域，理想中的LLM应该是领域无关的通用人工智能模型，它现在在某一两个领域做得好，不代表只能做这些任务。ChatGPT的出现，证明了现在这个时期，我们去追求AGI是有可行性的，而现在是抛开“领域学科”这个思维束缚的时候了。</p><p>ChatGPT除了展示出以流畅的对话形式解决各种NLP任务外，也具备强大的代码能力。很自然的，之后越来越多其它的研究领域，也会被逐步纳入LLM体系中，成为通用人工智能的一部分。</p><p><img src="https://pic2.zhimg.com/80/v2-d500cf5469a15e19066e227ac6e4763d_1440w.webp" alt="In-Context learning"></p><p>LLM从NLP向外进行领域拓展，一个自然的选择就是图像处理及多模态相关任务。目前已经有些工作在尝试把多模态融入，让LLM成为一个支持多模态输入输出的通用人机接口，典型的例子包括DeepMind的Flamingo和微软的“Language Models are General-Purpose Interfaces”，上图展示了这种方式的概念结构。</p><p>我的判断是无论是图像还是多模态，未来被融入LLM成为好用的功能，可能比我们想象的进度要慢。主要原因在于：尽管图像领域最近两年也一直在模仿Bert预训练的路子，尝试引入自监督学习，释放模型自主从图像数据中学习知识的能力，典型技术就是“对比学习”和MAE，这是两条不同的技术路线。然而，从目前效果来看，尽管取得了很大的技术进步，但貌似这条路尚未走通，这体现在图像领域预训练模型应用到下游任务，带来的效果收益，远不如Bert或GPT应用在NLP下游任务那样显著。所以，图像预处理模型仍需深入探索，以释放图像数据的潜力，而这会迟滞它们被统一到LLM大模型的时间。当然，如果哪天这条路被趟通，大概率会复现NLP领域目前的局面，就是图像处理各个研究子领域可能会逐步消失，被融入到大型LLM中来，直接完成终端任务。</p><p>除了图像与多模态，很明显，其它领域也会逐渐被纳入到理想LLM中来，这个方向方兴未艾，是具备高价值的研究主题。</p><p>以上是我对范式转换的个人思考，接下来，我们来梳理下GPT 3.0之后LLM模型的主流技术进展。如理想LLM模型所示，相关的技术其实可以分为两大类；一类是关于LLM模型如何从数据中吸收知识，也包括模型规模增长对LLM吸收知识能力带来的影响；第二类是关于人如何使用LLM内在能力来解决任务的人机接口，包括In Context Learning和Instruct两种模式。思维链（CoT）prompting这种LLM推理技术，本质上也属于In Context Learning，因为比较重要，我就把它们单独拎出来讲一下。</p><h2 id="学习者：从无尽数据到海量知识"><a href="#学习者：从无尽数据到海量知识" class="headerlink" title="学习者：从无尽数据到海量知识"></a>学习者：从无尽数据到海量知识</h2><p>从目前研究结果看，Transformer是足够强大的特征抽取器，尚不需要做特别的改进。那么通过预训练过程，Transformer学到了什么？知识是如何存取的？我们又如何修正错误知识？本节讲述这方面的研究进展。</p><h3 id="求知之路：LLM学到了什么知识"><a href="#求知之路：LLM学到了什么知识" class="headerlink" title="求知之路：LLM学到了什么知识"></a>求知之路：LLM学到了什么知识</h3><p>LLM从海量自由文本中学习了大量知识，如果把这些知识做粗略分类的话，可以分为语言类知识和世界知识两大类。</p><p>语言类知识指的是词法、词性、句法、语义等有助于人类或机器理解自然语言的知识。关于LLM能否捕获语言知识有较长研究历史，自从Bert出现以来就不断有相关研究，很早就有结论，各种实验充分证明LLM可以学习各种层次类型的语言学知识，这也是为何使用预训练模型后，各种语言理解类自然语言任务获得大幅效果提升的最重要原因之一。另外，各种研究也证明了浅层语言知识比如词法、词性、句法等知识存储在Transformer的低层和中层，而抽象的语言知识比如语义类知识，广泛分布在Transformer的中层和高层结构中。</p><p>世界知识指的是在这个世界上发生的一些真实事件（事实型知识，Factual Knowledge），以及一些常识性知识(Common Sense Knowledge)。比如“拜登是现任美国总统”、“拜登是美国人”、“乌克兰总统泽连斯基与美国总统拜登举行会晤”，这些都是和拜登相关的事实类知识；而“人有两只眼睛”、“太阳从东方升起”这些属于常识性知识。关于LLM模型能否学习世界知识的研究也有很多，结论也比较一致：LLM确实从训练数据中吸收了大量世界知识，而这类知识主要分布在Transformer的中层和高层，尤其聚集在中层。而且，随着Transformer模型层深增加，能够学习到的知识数量逐渐以指数级增加（可参考：BERTnesia: Investigating the capture and forgetting of knowledge in BERT）。其实，你把LLM看作是一种以模型参数体现的隐式知识图谱，如果这么理解，我认为是一点问题也没有的。</p><p>“When Do You Need Billions of Words of Pre-training Data?”这篇文章研究了预训练模型学习到的知识量与训练数据量的关系，它的结论是：对于Bert类型的语言模型来说，只用1000万到1亿单词的语料，就能学好句法语义等语言学知识，但是要学习事实类知识，则要更多的训练数据。这个结论其实也是在意料中的，毕竟语言学知识相对有限且静态，而事实类知识则数量巨大，且处于不断变化过程中。而目前研究证明了随着增加训练数据量，预训练模型在各种下游任务中效果越好，这说明了从增量的训练数据中学到的更主要是世界知识。</p><h3 id="知识涂改液：如何修正LLM里存储的知识"><a href="#知识涂改液：如何修正LLM里存储的知识" class="headerlink" title="知识涂改液：如何修正LLM里存储的知识"></a>知识涂改液：如何修正LLM里存储的知识</h3><p>既然我们已知具体的某条世界知识存储在某个或者某些FFN节点的参数里，自然会引发另外一个问题：我们能否修正LLM模型里存储的错误或者过时的知识呢？比如对于问题：“英国的现任首相是谁？”鉴于近年来英国首相频繁更迭，你猜LLM更倾向输出“鲍里斯”还是更青睐“苏纳克”？很明显训练数据中包含“鲍里斯”的数据会更多，这种情况很大可能LLM会给出错误回答，于是我们就有修正LLM里存储的过时知识的必要性。</p><p>如果归纳下，目前有三类不同方法来修正LLM里蕴含的知识：</p><p>第一类方法从训练数据的源头来修正知识。“Towards Tracing Factual Knowledge in Language Models Back to the Training Data”这篇文章的研究目标是：对于指定的某条知识，我们是否可以定位到是哪些训练数据导致LLM学会了这条知识？答案是肯定的，这意味着我们可以逆向追踪到某条知识对应的训练数据源头。如果利用这项技术，假设我们想要删除某条知识，则可首先定位到其对应的数据源头，删除数据源，然后重新预训练整个LLM模型，这样即可达成删除LLM中相关知识的目的。但是这里有个问题，如果修正一小部分知识，我们就需要重新做一次模型预训练，这样做明显成本太高。所以这种方法不会太有发展前景，可能比较适合那种对于某个特定类别数据的一次性大规模删除场合，不适合少量多次的常规知识修正场景，比如可能比较适合用来做去除偏见等去toxic内容的处理。</p><p>第二类方法是对LLM模型做一次fine-tuning来修正知识。一个直观能想到的方法是：我们可以根据要修正成的新知识来构建训练数据，然后让LLM模型在这个训练数据上做fine-tuning，这样指导LLM记住新的知识，遗忘旧的知识。这个方法简单直观，但是也有一些问题，首先它会带来灾难遗忘问题，就是说除了忘掉该忘的知识，还忘掉了不该忘的知识，导致这么做了之后有些下游任务效果下降。另外，因为目前的LLM模型规模非常大，即使是做fine-tuning，如果次数频繁，其实成本也相当高。对这种方法感兴趣的可以参考“Modifying Memories in Transformer Models”。</p><p>另外一类方法直接修改LLM里某些知识对应的模型参数来修正知识。假设我们想要把旧知识&lt;英国，现任首相，鲍里斯&gt;，修正到&lt;英国，现任首相，苏纳克&gt;。首先我们想办法在LLM模型参数中，定位到存储旧知识的FFN节点，然后可以强行调整更改FFN中对应的模型参数，将旧知识替换成新的知识。可以看出，这种方法涉及到两项关键技术：首先是如何在LLM参数空间中定位某条知识的具体存储位置；其次是如何修正模型参数，来实现旧知识到新知识的修正。关于这类技术的细节，可以参考“Locating and Editing Factual Associations in GPT”和“Mass-Editing Memory in a Transformer”。理解这个修正LLM知识的过程，其实对于更深入理解LLM的内部运作机制是很有帮助的。</p><h2 id="规模效应：当LLM越来越大时会发生什么"><a href="#规模效应：当LLM越来越大时会发生什么" class="headerlink" title="规模效应：当LLM越来越大时会发生什么"></a>规模效应：当LLM越来越大时会发生什么</h2><p>我们知道，近年来，LLM模型规模在快速增长，目前效果最好的LLM模型，其参数规模大都超过了千亿（100B）参数规模。比如，OpenAI的GPT 3的规模为175B，Google的LaMDA规模为137B，PaLM的规模为540B，DeepMind的Gogher规模为280B等，不一而足。国内也有中文巨型模型，比如清华&amp;智谱GLM规模130B，华为“盘古”规模200B，百度“文心”规模260B，浪潮“源1.0”规模245B。那么，一个很自然的问题就是：随着LLM模型规模不断增长，会发生些什么呢？</p><p>预训练模型的应用往往是两阶段的：预训练阶段，及具体场景应用阶段。在预训练阶段，其优化目标是交叉熵，对GPT这种自回归语言模型来说，也就是看LLM是否正确预测到了下一个单词；而场景应用阶段，一般要看具体场景的评价指标。一般我们的直觉是：如果LLM模型在预训练阶段的指标越好，自然它解决下游任务的能力就越强。然而，事实并非完全如此。现有研究已证明，预训练阶段的优化指标确实和下游任务表现出正相关关系，但是并非完全正相关。也就是说，只看预训练阶段的指标，来判断一个LLM模型是否够好，这是不够的。基于此，我们分头来看在这两个不同阶段，随着LLM模型增大，有什么影响。</p><p><img src="https://pic1.zhimg.com/80/v2-d3de4b481e4944000963a5564cd6745c_1440w.webp"></p><p>首先，我们先看在预训练阶段，随着模型规模逐步增大，会发生什么。OpenAI在“Scaling Laws for Neural Language Models”中专门研究了这个问题，并提出LLM模型所遵循的“伸缩法则”（scaling law）。如上图所示，这个研究证明：当我们独立增加训练数据量、模型参数规模或者延长模型训练时间（比如从1个Epoch到2个Epoch），预训练模型在测试集上的Loss都会单调降低，也就是说模型效果越来越好。</p><p>既然三个因素都重要，那么我们在实际做预训练的时候，就有一个算力如何分配的决策问题：假设用于训练LLM的算力总预算（比如多少GPU小时或者GPU天）给定，那么是应该多增加数据量、减少模型参数呢？还是说数据量和模型规模同时增加，减少训练步数呢？此消彼长，某个要素规模增长，就要降低其它因素的规模，以维持总算力不变，所以这里有各种可能的算力分配方案。最终OpenAI选择了同时增加训练数据量和模型参数，但是采用早停策略(early stopping)来减少训练步数的方案。因为它证明了：对于训练数据量和模型参数这两个要素，如果只单独增加其中某一个，这不是最好的选择，最好能按照一定比例同时增加两者，它的结论是优先增加模型参数，然后才是训练数据量。假设用于训练LLM的算力总预算增加了10倍，那么应该增加5.5倍的模型参数量，1.8倍的训练数据量，此时模型效果最佳。</p><p>DeepMind的一项研究（参考：Training Compute-Optimal Large Language Models）更深入地探究了这个问题，其基本结论和OpenAI的结论差不多，比如确实需要同时增加训练数据量和模型参数，模型效果才会更好。而很多大模型在做预训练的时候，并没有考虑这一点，很多LLM大模型只是单调增加模型参数，而固定住了训练数据量，这个做法其实是不对的，限制了LLM模型的潜力。但是它修正了两者的比例关系，认为训练数据量和模型参数是同等重要的，也就是说，假设用于训练LLM的算力总预算增加了10倍，那么应该增加3.3倍的模型参数量，3.3倍的训练数据量，这样模型效果才最好。</p><p>这意味着：增加训练数据量的重要性，比我们之前所认为的，还要重要。基于这个认知，DeepMind在设计Chinchilla模型时，在算力分配上选择了另外一种配置：对标数据量300B、模型参数量280B的Gopher模型，Chinchilla选择增加4倍的训练数据，但是将模型参数降低为Gopher的四分之一，大约为70B。但是无论预训练指标，还是很多下游任务指标，Chinchilla效果都要优于规模更大的Gopher。</p><p>这带给我们如下启示：我们可以选择放大训练数据，并同比例地减少LLM模型参数，以达到在不降低模型效果的前提下，极大缩小模型规模的目的。缩小模型规模有很多好处，比如在应用的时候，推理速度会快很多等，无疑这是一个很有前途的LLM发展路线。</p><p>以上是从预训练阶段来看模型规模的影响，如果从LLM解决下游具体任务效果的角度来看，随着模型规模增大，不同类型的任务有不同的表现，具体而言，有以下三类情况。</p><p><img src="https://pic2.zhimg.com/80/v2-baea419ed3bb60c706f92459a968debd_1440w.webp"></p><p>第一类任务完美体现了LLM模型的scaling law，就是说随着模型规模逐步放大，任务的表现越来越好，如上图里的（a）图所示。这类任务通常符合如下共性：它们往往都是知识密集型任务，也就是说如果LLM模型包含的知识量越多，这类任务表现越好。而很多研究已经证明越大的LLM模型学习效率越高，也就是说相同训练数据量，模型越大任务效果越好，说明面对的即使是同样的一批训练数据，更大的LLM模型相对规模小一些的模型，从中学到了更多的知识。更何况一般情况下，在增大LLM模型参数的时候，往往会同步增加训练数据量，这意味着大模型可以从更多数据中学习更多的知识点。这些研究可以很好地解释上图，为何随着模型规模增大，这些知识密集型的任务效果越来越好。大多数传统的自然语言理解类任务，其实都属于这种知识密集型任务，而很多任务在近两年获得了极大的效果提升，甚至超过了人类表现。很明显，这大概率是LLM模型的规模增长带来的，而非归功于某项具体的技术改进。</p><p>第二类任务展现出LLM具备某种“涌现能力（Emergent Ability）”，如上图（b）所示。所谓“涌现能力”，指的是当模型参数规模未能达到某个阀值时，模型基本不具备解决此类任务的任何能力，体现为其性能和随机选择答案效果相当，但是当模型规模跨过阀值，LLM模型对此类任务的效果就出现突然的性能增长。也就是说，模型规模是解锁(unlock)LLM新能力的关键，随着模型规模越来越大，会逐渐解锁LLM越来越多的新能力。这是个很神奇的现象，因为它意味着如下让人对未来可报乐观预期的可能：或许很多任务，目前LLM还不能很好地解决，甚至站在现在这个时刻的我们看起来，LLM完全没有能力解决这类任务，但因LLM具备“涌现能力”，所以如果我们继续推大模型，也许某一天它的这项能力就被突然解锁了。LLM模型的规模增长会给我们带来意想不到的精彩礼物。</p><p>“Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models”这篇文章指出，这类体现出“涌现能力”的任务也有一些共性：这些任务一般由多步骤构成，要解决这些任务，往往需要先解决多个中间步骤，而逻辑推理能力在最终解决这类任务中发挥重要作用。思维链（Chain of Thought）Prompting是典型的增强LLM推理能力的技术，能大幅提升此类任务的效果，关于CoT技术，在随后小节内容会做解释，此处暂不展开。</p><p>问题是，为何LLM会出现这种“涌现能力”现象呢？上述文章以及“Emergent Abilities of Large Language Models”给出了几个可能的解释：</p><p>一种可能解释是有些任务的评价指标不够平滑。比如说有些生成任务的判断标准，它要求模型输出的字符串，要和标准答案完全匹配才算对，否则就是0分。所以，即使随着模型增大，其效果在逐步变好，体现为输出了更多的正确字符片段，但是因为没有完全对，只要有任何小错误都给0分，只有当模型足够大，输出片段全部正确才能得分。也就是说，因为指标不够平滑，所以不能体现LLM其实正在逐步改善任务效果这一现实，看起来就是“涌现能力”这种外在表现。</p><p>另外一种可能的解释是：有些任务由若干中间步骤构成，随着模型规模增大，解决每个步骤的能力也在逐步增强，但是只要有一个中间步骤是错的，最终答案就是错的，于是也会导致这种表面的“涌现能力”现象。</p><p>当然，上面的解释目前还都是猜想，至于为何LLM会出现这种现象，还需要进一步更深入的研究。</p><p><img src="https://pic1.zhimg.com/80/v2-2fe694b838879c02f2f4b9a2375c680c_1440w.webp"></p><p>还有少部分任务，随着模型规模增长，任务的效果曲线展现出U形特性：随着模型规模逐渐变大，任务效果逐渐变差，但是当模型规模进一步增长，则效果开始越来越好，呈现出U形增长趋势，如上图所示的粉红色PaLM模型在两个任务上的指标走势。为何这些任务表现得如此特殊呢？“Inverse scaling can become U-shaped”这篇文章给出了一种解释：这些任务，内部其实隐含了两种不同类型的子任务，一种是真正的任务，另外一种是“干扰任务（distractor task）”。当模型规模小的时候，无法识别任意一种子任务，所以模型的表现跟随机选择答案差不多，当模型增长到中等规模的时候，主要执行的是干扰任务，所以对真正的任务效果有负面影响，体现为真正任务效果的下降，而当进一步增加模型规模，则LLM可以忽略干扰任务，执行真正的任务，体现为效果开始增长。</p><p>对于那些随着模型规模增大，效果一直下降的任务，如果采用思维链（CoT）Prompting，则部分任务的表现转换为遵循Scaling law，即模型规模越大效果越好，而其它任务则转换为U性增长曲线。这其实侧面说明了：此类任务应属于推理类型的任务，所以加入CoT后任务表现会发生质的变化。</p><h2 id="人机接口-从In-Context-Learning到Instruct理解"><a href="#人机接口-从In-Context-Learning到Instruct理解" class="headerlink" title="人机接口:从In Context Learning到Instruct理解"></a>人机接口:从In Context Learning到Instruct理解</h2><p>一般我们经常提到的人和LLM的接口技术包括：zero shot prompting、few shot prompting、In Context Learning，以及Instruct。这些其实都是表达某个具体任务的描述方式。不过如果你看文献，会发现叫法比较乱。</p><p>其中Instruct 是ChatGPT的接口方式，就是说人以自然语言给出任务的描述，比如“把这个句子从中文翻译成英文”，类似这种。zero shot prompting我理解其实就是现在的Instruct的早期叫法，以前大家习惯叫zero shot，现在很多改成叫Instruct。尽管是一个内涵，但是具体做法是两种做法。早期大家做zero shot prompting，实际上就是不知道怎么表达一个任务才好，于是就换不同的单词或者句子，反复在尝试好的任务表达方式，这种做法目前已经被证明是在拟合训练数据的分布，其实没啥意思。目前Instruct的做法则是给定命令表述语句，试图让LLM理解它。所以尽管表面都是任务的表述，但是思路是不同的。</p><p>而In Context Learning和few shot prompting意思类似，就是给LLM几个示例作为范本，然后让LLM解决新问题。我个人认为In Context Learning也可以理解为某项任务的描述，只是Instruct是一种抽象的描述方式，In Context Learning是一种例子示范的例子说明法。当然，鉴于目前这几个叫法用的有点乱，所以上述理解仅代表个人看法。</p><p>所以我们此处只对In Context Learning和Instruct进行介绍，不再提zero shot和few shot了。</p><h3 id="神奇的Instruct理解"><a href="#神奇的Instruct理解" class="headerlink" title="神奇的Instruct理解"></a>神奇的Instruct理解</h3><p>我们可以把Instruct当作一种方便人类理解的任务表述，在这个前提下，目前关于Instruct的研究可以分成两种：偏学术研究的Instruct，以及关于人类真实需求描述的Instruct。</p><p><img src="https://pic1.zhimg.com/80/v2-7be5bf5949527def595cf297d494e750_1440w.webp"></p><p>我们先来看第一种：偏学术研究的Instruct。它的核心研究主题是多任务场景下，LLM模型对Instruct理解的泛化能力。如上图中FLAN模型所示，就是说有很多NLP任务，对于每个任务，研究人员构造一个或者多个Prompt模版作为任务的Instruct，然后用训练例子对LLM模型进行微调，让LLM以同时学习多个任务。训练好模型后，给LLM模型一个它没见过的全新任务的Instruct，然后让LLM 解决zero shot任务，从任务解决得是否足够好，来判断LLM模型是否有对Instruct理解的泛化能力。</p><p>如果归纳下目前的研究结论（可参考“Scaling Instruction-Fine-tuned Language Models”／“Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks”），能够有效增加LLM模型Instruct泛化能力的因素包括：增加多任务的任务数量、增加LLM模型大小、提供CoT Prompting， 以及增加任务的多样性。如果采取任意一项措施，都可以增加LLM模型的Instruct理解能力。</p><p>第二种是人类真实需求下的Instruct，这类研究以InstructGPT和ChatGPT为代表。这类工作也是基于多任务的，但是和偏向学术研究类工作最大的不同，在于它是面向人类用户真实需求的。为什么这么说呢？因为它们用于LLM多任务训练的任务描述Prompt，是从大量用户提交的真实请求中抽样而来的，而不是固定好研究任务的范围，然后让研究人员来写任务描述prompt。这里所谓的“真实需求”，体现在两个方面：首先，因为是从用户提交的任务描述里随机抽取的，所以涵盖的任务类型更多样化，也更符合用户的真实需求；其次，某个任务的prompt描述，是用户提交的，体现了一般用户在表达任务需求时会怎么说，而不是你认为用户会怎么说。很明显，这类工作改出来的LLM模型，用户体验会更好。</p><p>InstructGPT论文里，也拿这种方法和FLAN那种Instruct based方法做了比较。首先在GPT3上用FLAN提到的任务、数据以及Prompt模版进行微调，来在GPT 3上复现FLAN方法，然后和InstructGPT进行比较，因为InstructGPT的基础模型也是GPT3，所以只有数据和方法的差别，两者可比，结果发现FLAN方法的效果，距离InstructGPT有很大的差距。那么背后的原因是什么呢？论文分析数据后认为，FLAN方法涉及到的任务领域相对少，是InstructGPT涉及领域的子集，所以效果不好。也就是说，FLAN论文里涉及到的任务和用户真实需求是不符的，而这导致在真实场景下效果不够好。而这对我们的启示是：从用户数据中收集真实需求，这事情是很重要的。</p><h3 id="In-Context-Learning和Instruct的联系"><a href="#In-Context-Learning和Instruct的联系" class="headerlink" title="In Context Learning和Instruct的联系"></a>In Context Learning和Instruct的联系</h3><p>如果我们假设In Context Learning是用一些例子来具象地表达任务命令，Instruct是一种更符合人类习惯的抽象任务描述。那么，一个很自然的问题是：它们之间有什么联系吗？比如，我们是否能够提供给LLM完成某个任务的若干具体示例，让LLM找出其对应的自然语言描述的Instruct命令？</p><p><img src="https://pic2.zhimg.com/80/v2-d232352a4e6cb150956c5e096c41b3d5_1440w.webp"></p><p>目前有零星的工作在探索这个问题，我认为这个方向是很有研究价值的。先说答案，答案是：Yes，LLM Can。“Large Language Models Are Human-Level Prompt Engineers”是做这个方向很有趣的工作，如上图所示，对于某项任务，给LLM一些示例，让LLM自动生成能够描述这项任务的自然语言命令，然后它再用LLM生成的任务描述去测试任务效果。它使用的基础模型是GPT 3和InstructGPT，经过这项技术加持后，LLM生成的Instruct的效果相比未采用这项技术的GPT 3 以及InstuctGPT来说，指标有极大地提升，而且在一些任务上超过人类的表现。</p><p>这说明了：具象的任务示例和任务的自然语言描述之间，有种神秘的内在联系。至于这种联系到底是什么？我们目前对此还一无所知。</p><h2 id="智慧之光：如何增强LLM的推理能力"><a href="#智慧之光：如何增强LLM的推理能力" class="headerlink" title="智慧之光：如何增强LLM的推理能力"></a>智慧之光：如何增强LLM的推理能力</h2><p>目前很多研究已证明LLM对于知识具有强大的记忆能力，但是，一般我们不会因为一个人记忆能力强，就说这人很聪明，是否具有强大的推理能力，往往是我们判断一个人是否聪明的重要标准。类似的，如果LLM的效果想让人觉得很惊艳，强大的推理能力是必备的。推理能力本质上是综合运用很多相关知识点，去推导出新知识或新结论。关于LLM的推理能力，是最近一年来LLM里最重要和热门的研究领域之一。于是，我们关心的问题就是：LLM具备推理能力吗？如果具备，那么它的推理能力够强吗？</p><p>这两个问题目前的答案似乎应该是：当模型规模足够大的时候，LLM本身是具备推理能力的，在简单推理问题上，LLM已经达到了很好的能力，但是复杂推理问题上，还需要更多深入的研究。</p><p>如果梳理现有LLM推理相关工作的话，我把它们归到两大类，体现出挖掘或促进LLM推理能力不同的技术思路：第一类研究比较多，可以统称为基于Prompt的方法，核心思想是通过合适的提示语或提示样本，更好地激发出LLM本身就具备的推理能力，Google在这个方向做了大量很有成效的工作。第二类做法是在预训练过程中引入程序代码，和文本一起参与预训练，以此进一步增强LLM的推理能力，这应该是OpenAI实践出的思路。比如ChatGPT肯定具备很强的推理能力，但它并不要求用户必须提供一些推理示例，所以ChatGPT强大的推理能力，大概率来源于使用代码参与GPT 3.5的预训练。</p><p>这两种思路其实大方向是迥异的：利用代码增强LLM推理能力，这体现出一种通过增加多样性的训练数据，来直接增强LLM推理能力的思路；而基于Prompt的方法，它并不会促进LLM本身的推理能力，只是让LLM在解决问题过程中更好地展示出这种能力的技术方法。可以看出，前者（代码方法）治本，后者治标。当然，两者其实也是互补的，但从长远看，治本的方法更重要。</p><h3 id="基于Prompt的方法"><a href="#基于Prompt的方法" class="headerlink" title="基于Prompt的方法"></a>基于Prompt的方法</h3><p>这方面工作非常多，如果归纳一下的话，大致可以分为三条技术路线。</p><p><img src="https://pic3.zhimg.com/80/v2-f32d65415756467036154c4d4bf9882e_1440w.webp"></p><p>第一种思路是直接在问题上追加辅助推理Prompt。这种方法简单直接，但在众多领域都很有效。这个做法是由“Large language models are zero-shot reasoners”提出的，也被称为zero-shot CoT。具体而言，分为两个阶段（如上图所示），第一阶段在提问的问题上追加“Let’s think step by step”这句提示语，LLM会输出具体的推理过程；第二阶段，在第一阶段的问题后，拼接LLM输出的具体推理过程，并再追加Prompt&#x3D;“Therefore, the answer (arabic numerals) is”，此时LLM会给出答案。如此简单的操作，却可以大幅增加LLM在各项推理任务中的效果，比如在数学推理测试集GSM8K上，加上提示语后，推理准确率直接从原先的10.4%提升到了40.4%，可谓神奇。</p><p>为什么LLM会具备给一句“Let’s think step by step”提示语，就能列出详细的推理步骤并算出答案呢？其原因目前尚无定论，我的猜测是：很可能因为预训练数据里面存在大量的此种数据，就是以“Let’s think step by step”开头，然后后面是详细的推理步骤，最后给出答案，而LLM在预训练的时候记住了这些模式。而当我们输入这个提示语的时候，激发LLM模糊得“回忆”起某些例子的推导步骤，于是即可模仿这些例子进行步骤推理并给出答案。当然这只是我的无依据推论，若事实真的如此，如果你看过后面介绍的标准CoT做法，会发现Zero-shot CoT 本质上和标准CoT很可能没什么区别，只是标准CoT由人工来写推理步骤的示例，而Zero-shot CoT大概率是通过提示语，激活了记忆中的某些包含推理步骤的示例，很可能是如此区别。而标准CoT效果比Zero-Shot CoT效果好也完全可以理解，因为毕竟靠LLM回忆示例，精准性估计不会太高，而人工给出的示例，准确性是有保障的，所以自然标准CoT效果会更好。</p><p>这侧面说明了一个道理，就是LLM本身是具备推理能力的，只是我们没有办法把它的这种能力激发出来而已，通过合适的提示语来进行两步提示，就在一定程度上可以释放出它的这种潜力。另外，对于中文，很可能存在另外一个黄金提示语，比如“详细解题思路如下”，类似这种，因为中文语料在讲解推理步骤的时候，经常用的引导句和“让我们一步一步来思考”应该是不同的，这是明显的西方说法，而探索出这个中文黄金提示语，其实也是很有必要的。</p><p>第二种思路一般被称为基于示例的思维链（few-shot CoT,Chain of Thought）Prompting。这个方向目前是LLM推理研究的主方向，很多工作都是在这个思路上做的，我们简单介绍几个效果显著的代表性工作，基本能代表CoT的技术发展方向。</p><p><img src="https://pic4.zhimg.com/80/v2-c0d021d95016c243f087d599fed9be03_1440w.webp"></p><p>CoT的主体思想其实很直白；为了教会LLM模型学会推理，给出一些人工写好的推理示例，示例里把得到最终答案前，一步步的具体推理步骤说清楚，而这些人工写的详细推理过程，就是思维链Prompting，具体例子可参照上图中蓝色文字部分。CoT的意思是让LLM模型明白一个道理；就是在推理过程中，步子不要迈得太大，否则很容易出错，改变思维模式，化大问题为小问题，步步为营，积小胜为大胜。最早明确提出CoT这个概念的文章是“Chain of thought prompting elicits reasoning in large language models”，论文发布于22年1月份，虽然做法很简单，但是应用CoT后LLM模型的推理能力得到了巨大提升，GSM8K数学推理测试集准确率提高到60.1%左右。当然，这种给出详细推理步骤和中间过程的思想，并非CoT最早提出的，更早一些的“scratchpad”技术（可参考：Show Your Work: Scratchpads for Intermediate Computation with Language Models）首先采用了类似的思路。</p><p><img src="https://pic4.zhimg.com/80/v2-37f3affd49d4fb16436de309e514af3f_1440w.webp"></p><p>CoT提出不久，很快在22年3月份，一项被称为“Self-Consistency”的改进技术就将GSM8K测试集准确率提高到74.4%，提出这项改进的论文是“Self-Consistency Improves Chain of Thought Reasoning in Language Models”。“Self-Consistency”的思路也很直观（参考上图）：首先可以利用CoT给出几个写了推理过程的示例，然后要求LLM对给定的问题进行推理，如果是CoT，直接输出一个推理过程和答案，整个过程就结束了。“Self-Consistency”则不然，它要求LLM输出多个不同的推理过程和答案，然后采用投票的方式选出最佳答案，思路非常简单直接，但是效果也确实好。“Self-Consistency”其实是教导LLM学会这么一个道理：孔乙己说过茴香豆的“茴”字有四种写法，类似的，一个数学题的正确解法也可以有很多种，每个不同的推导过程都指向最终的答案。条条大路通罗马，虽说也有个别迷路走到北京的，但是迷路的毕竟是少数，看看大多数人走到哪里，哪里就是正确答案。简单的方法往往蕴含着深刻的哲学含义，是不是这道理？</p><p>再往后，“On the Advance of Making Language Models Better Reasoners”这个工作在“Self-Consistency”基础上，进一步集成了“从一个Prompt问题拓展到多个Prompt问题、检查推理中间步骤的正确性以及对多个输出的回答加权投票”这三个改进点，将GSM8K测试集准确率提高到83%左右。</p><p><img src="https://pic2.zhimg.com/80/v2-bf7bf1c65038e7dc6d93d10fee3653e1_1440w.webp"></p><p>第三种思路体现了一种分治算法的思想。当然这个所谓“分治”是我归纳的，别人没这么说。这种思路的核心思想是：对于一个复杂的推理问题，我们把它分解成若干容易解决的子问题，一一解决掉子问题后，我们再从子问题的答案推导复杂问题的答案。你看这确实比较类似分治算法的思想吧。我个人觉得，这种思路可能才是揭示问题本质、最终解决LLM复杂推理问题正宗的道路。我们以“Least-to-most prompting”技术为例来说明这种思路的一种具体实现方式，如上图所示：它分为两个阶段，第一个阶段，从原始问题我们可以得知最终要问的问题是什么，我们假设最终问题是Final Q，然后从原始问题填充Prompt模版：“如果要解决Final Q问题，那么我需要先解决”，然后把原始问题和这个Prompt交给LLM，让LLM模型给出答案，等于让LLM给出最终问题的前置子问题Sub Q；接下来我们进入第二个阶段，让LLM先回答刚才拿到的子问题Sub Q，并拿到对应的答案，然后原始问题拼接子问题Sub Q及对应答案，再去问LLM最终那个问题Final Q，此时LLM会给出最后的答案。如此这般，体现出拆解子问题，并从子问题的答案逐步找出最终答案的思路。</p><h3 id="代码预训练增强LLM推理能力"><a href="#代码预训练增强LLM推理能力" class="headerlink" title="代码预训练增强LLM推理能力"></a>代码预训练增强LLM推理能力</h3><p>以上是目前利用Prompt激发LLM模型推理能力的三种主流做法，而关于LLM的推理能力，目前还观察到一个有趣且费解的现象：除了文本外，如果能够加入程序代码一起参与模型预训练，则能大幅提升LLM模型的推理能力。这个结论从不少论文的实验部分都可以得出（可以参考：AUTOMATIC CHAIN OF THOUGHT PROMPTING IN LARGE LANGUAGE MODELS／Challenging BIG-Bench tasks and whether chain-of-thought can solve them等论文的实验部分）。</p><p><img src="https://pic1.zhimg.com/80/v2-addaa608efb30c310a9f7155d5c5eb20_1440w.webp"></p><p>上图给出了一份实验数据，来自于论文“On the Advance of Making Language Models Better Reasoners”，其中GPT3 davinci就是标准的GPT 3模型，基于纯文本训练；code-davinci-002（OpenAI内部称为Codex）是同时在Code和NLP数据上训练的模型。如果比较两者效果，可以看出，不论采用具体哪种推理方法，仅仅是从纯文本预训练模型切换到文本和Code混合预训练模型，在几乎所有测试数据集合上，模型推理能力都得到了巨大的效果提升，比如我们以“Self Consistency”方法为例，在大多数据集合上的性能提升，都直接超过了20到50个百分点，这是很恐怖的性能提升，而其实在具体推理模型层面，我们什么也没做，仅仅是预训练的时候除了文本，额外加入了程序代码而已。</p><p>除了这个现象，从上图数据中，我们还可以得出其它一些结论，比如GPT 3这种纯文本预训练模型，其实是具备相当程度的推理能力的，除了在GSM8K这种数学推理上效果比较差外，其它推理数据数据集合表现也还可以，前提你需要采用合适的方法，来激发出它本身就具备的这种能力；再比如，text-davinci-002，也就是在code-davinci-002基础上加入instruct fine-tuning后的模型（就是加入InstructGPT或ChatGPT模型的第一步），其推理能力要弱于Codex，但是有其它研究表明它在自然语言处理任务又要强于Codex。而这貌似说明了，加入instruct fine-tuning，会损害LLM模型的推理能力，但是会在一定程度上提升自然语言理解能力。而这些结论其实都是很有意思的，也能启发后续进一步的思考和探索。</p><p>那么，一个自然的疑问是：为何预训练模型可以从代码的预训练中获得额外的推理能力？确切原因目前未知，值得深入探索。我猜测可能是因为原始版本的Codex（只使用代码训练，可参考文献：Evaluating Large Language Models Trained on Code）的代码训练是从文本生成代码，而且代码中往往包含很多文本注释，本质上这类似于预训练模型做了&lt;文本,Code&gt;两种数据的多模态对齐工作。而数据中必然包含相当比例的数学或逻辑问题的代码、描述和注释，很明显这些数学类或逻辑推理类的数据，对于解决下游数学推理问题是有帮助的，我猜大概率原因在此。</p><h3 id="关于LLM推理能力的思考"><a href="#关于LLM推理能力的思考" class="headerlink" title="关于LLM推理能力的思考"></a>关于LLM推理能力的思考</h3><p>上面介绍了LLM推理的主流技术思路和现有的一些结论，接下来谈谈我对LLM模型推理技术的思考，以下内容纯个人推断，没有太多证据，还请谨慎参考。我的判断是：虽然最近一年来，关于激发LLM的推理能力，这方面的技术进展很快，也取得了很大的技术进步，但是总体感觉是，我们可能走在正确的方向上，但是距离接触到真正的问题本质还有一段距离，对此要有更深入的思考和探索。</p><p>首先，我比较赞同上述分治算法的主体思路，对于复杂的推理问题，我们应该把它拆解成若干简单的子问题，因为子问题对于LLM来说回答正确的概率就大很多，让LLM一一回答子问题后，再逐步推导出最终答案。受到“Least-to-most prompting”技术的启发，如果进一步思考，我觉得LLM推理本质上很可能会是如下两种可能的其中之一：不断和LLM进行交互的图上推理问题，抑或是不断和LLM进行交互的程序流程图执行问题。</p><p><img src="https://pic1.zhimg.com/80/v2-5098b630deb9eb5fd3ff89d0bc17d364_1440w.webp"></p><p>先说图上推理问题，如上图所示，假设我们有办法能够把复杂问题拆解成由子问题或者子步骤构成的图结构，图中的节点是子问题或者子步骤，图中的边代表了子问题之间的依赖关系，就是说只有回答好子问题A，才能回答子问题B，而且图中大概率存在循环结构，就是反复做某几个子步骤。假设我们能够得到上述的子问题拆解图，那么可以根据依赖关系，引导LLM一步一步按照图结构，回答必须首先回答的子问题，直到推导出最终答案。</p><p><img src="https://pic3.zhimg.com/80/v2-3536458162994a5b33a02147354d915a_1440w.webp"></p><p>再说程序流程图问题，参考上图，假设我们有办法把复杂问题拆解成子问题或子步骤，并产生一个由子步骤构成的类似程序流程图的结构，在这个结构里，有些步骤会反复执行多次（循环结构），有些步骤的执行需要进行条件判断（条件分支）。总而言之，在执行每个子步骤的时候和LLM进行交互，得到子步骤的答案，然后按照流程不断执行，直到输出最终答案。类似这种模式。假设这个思路大致正确的话，也许可以从这个角度来解释为何加入代码会增强预训练模型的推理能力：大概率因为&lt;文本，代码&gt;的多模态预训练模型，在模型内部是通过类似这种隐含的程序流程图作为两个模态的桥梁，将两者联系起来的，即由文本描述到隐含的流程图，再映射到由流程图产生具体的代码。也就是说，这种多模态预训练，可以增强LLM模型从文本构建出隐含的流程图并按照流程图执行的能力，也就是加强了它的推理能力。</p><p>当然，上述思路最大的问题是，我们如何根据文本描述的问题，能够靠LLM模型，或者其它模型，得到图结构或者流程图结构？这个可能是其中的难点。一种可能的思路就类似继续增强文本和更高质量的代码预训练，走隐式学习内部隐含结构的方法。而目前的CoT技术，如果套到上述思路来思考的话，可以这么理解：标准CoT，其实就是靠自然语言文本来描述图结构或者程序流程图的；而“Least-to-most prompting”技术，则是试图根据最后一个图节点，靠倒推来试图推导出其中的图结构，但是很明显，目前的方法限制了它倒推的深度，也就是说它只能推导出非常简单的图结构，这正是限制它能力的所在。</p><h2 id="未来之路：LLM研究趋势及值得研究的重点方向"><a href="#未来之路：LLM研究趋势及值得研究的重点方向" class="headerlink" title="未来之路：LLM研究趋势及值得研究的重点方向"></a>未来之路：LLM研究趋势及值得研究的重点方向</h2><p>这里列出一些我个人认为比较重要的LLM研究领域，或值得深入探索的研究方向。</p><p><strong>探索LLM模型的规模天花板</strong></p><p>尽管继续推大LLM模型的规模，这事看似没有技术含量，但是其实这个事情异常重要。我个人判断，自从Bert出现以来，到GPT 3，再到ChatGPT，大概率这些给人印象深刻的关键技术突破，核心贡献都来自于LLM模型规模的增长，而非某项具体技术。说不定，揭开AGI真正的钥匙就是：超大规模及足够多样性的数据、超大规模的模型，以及充分的训练过程。再者，做超大规模的LLM模型，对技术团队的工程实现能力要求是非常高的，也不能认为这事情缺乏技术含量。</p><p>那么继续推大LLM模型规模，有什么研究意义呢？我觉得有两方面的价值。首先，如上所述，我们已知，对于知识密集型的任务，随着模型规模越大，各种任务的效果会越来越好；而对很多推理类型的有难度的任务，加上CoT Prompting后，其效果也呈现出遵循Scaling law的趋向。那么，很自然的一个问题就是：对于这些任务，LLM的规模效应，能将这些任务解决到何种程度？这是包括我在内，很多人关心的问题。其次，考虑到LLM具备的神奇的“涌现能力”，如果我们继续增加模型规模，它会解锁哪些让我们意想不到的新能力呢？这也是很有意思的问题。考虑到以上两点，我们仍然需要不断增大模型规模，看看模型规模对解决各类任务的天花板在哪里。</p><p>当然，这种事情也就只能说说，对99.99%的从业者来说，是没有机会和能力做这个事情的。要做这个事情，对研究机构的财力及投入意愿、工程能力、技术热情，都有极高的要求，缺一不可。能做这事情的机构，粗估下来，国外不超过5家，国内不超过3家。当然，考虑到成本问题，未来也许会出现“股份制大模型”，就是有能力的几家机构合作，群策群力，一起来共建超级大模型的现象。</p><p><strong>增强LLM的复杂推理能力</strong></p><p>正如之前对LLM推理能力的叙述，尽管LLM在最近一年推理能力得到了很大的提升，但是很多研究（参考：Limitations of Language Models in Arithmetic and Symbolic Induction／Large Language Models Still Can’t Plan）表明，目前LLM能够解决得比较好的推理问题，往往都相对简单，LLM的复杂推理能力仍然薄弱，比如即使是简单的字符拷贝推理或者加减乘除运算，当字符串或者数字非常长的时候，LLM推理能力会极速下降，再比如行为规划能力等复杂推理能力很弱。总而言之，加强LLM的复杂推理能力，应该是LLM未来研究中最重要的环节之一。</p><p>前文有述，加入代码加入预训练，这是一种直接增强LLM推理能力的方向。这个方向目前研究尚显不足，更像是实践经验的总结，探索背后的原理，并进而引入更多类型除代码外的新型数据来增强LLM的推理能力，这可能是更本质提升推理能力的方向。</p><p><strong>LLM纳入NLP之外更多其它研究领域</strong></p><p>目前的ChatGPT擅长NLP和Code任务，作为通向AGI的重要种子选手，将图像、视频、音频等图像与多模态集成进入LLM，乃至AI for Science、机器人控制等更多、差异化更明显的其它领域逐步纳入LLM，是LLM通往AGI的必经之路。而这个方向才刚刚开始，因此具备很高的研究价值。</p><p><strong>更易用的人和LLM的交互接口</strong></p><p>如前所述，ChatGPT的最大技术贡献即在此。但是很明显，目前的技术并不完美，肯定还有很多命令LLM理解不了。所以，沿着这个方向，寻找更好的技术，来让人类使用自己习惯的命令表达方式，而LLM又能听懂，这是个新的，且非常有前景的技术方向。</p><p><strong>建设高难度的综合任务评测数据集</strong></p><p>好的评测数据集，是引导技术不断进步的基石。随着LLM模型逐步增大，任务效果快速提升，导致很多标准测试集快速过时。也就是说，这些数据集合相对现有技术来说，太容易了，在没有难度的测试集合下，我们不知道目前技术的缺陷和盲点在哪里。所以构建高难度的测试集合，是促进LLM技术进步的关键所在。</p><p>目前行业应出现了一些新的测试集，有代表性的包括BIGBench、OPT-IML等。这些测试集合体现出一些特性，比如相对LLM现有技术具备一定的难度、综合了各种各样多种类型的任务等。</p><p>受到ChatGPT的启发，我觉得除此外应纳入另一考虑因素：体现真实用户需求。就是说，这些任务的表述由用户真实发起，这种方式构建出来的LLM模型，才能解决用户实际需求。</p><p>除此外，相信LLM会快速将能力溢出到NLP之外的领域，而如何融入更多其它领域的评测数据，也是需要提前去考虑。</p><p><strong>高质量数据工程</strong></p><p>对于预训练模型来说，数据是其根本，预训练过程可以理解为从数据中吸取其中所包含知识的过程。因此，我们需要进一步加强对高质量数据的挖掘、收集及清洗等工作。</p><p>关于数据，需要考虑两个方面：数据的质量和数量。而根据T5的对比实验，我们可以得出结论：在数量和质量两个因素里，质量优先，正确的道路应该是在保证数据质量的前提下，再去增大数据规模。</p><p>数据质量，包括数据的信息含量以及数据的多样性等多个衡量标准，比如Wiki明显就属于世界知识密度极高的高质量数据，这是从信息含量来说的；而增加数据类型的多样性，无疑是激发LLM各种新能力的根本，比如加入问答网站的数据，对于LLM的QA能力提升是有直接帮助的。多样化的数据赋予了LLM更好解决更多不同类型任务的能力，所以，这可能是数据质量里最关键的标准。</p><p>关于数据数量，原则上互联网上公开发布的数据都可以纳入LLM模型的预训练过程。那么，它的极限在哪里？“Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning” 对此进行了估算，结论是到2026年左右，高质量的NLP数据将会用光，低质量NLP数据会在2030到2050年用光，而低质量图像数据会在2030到2060年用光。而这意味着：要么到时我们有新类型的数据源，要么我们必须增加LLM模型对数据的利用效率。否则，目前这种数据驱动的模型优化方式将会停止进步，或者收益减少。</p><p><strong>超大LLM模型Transformer的稀疏化</strong></p><p>目前规模最大的LLM中，有相当比例的模型采取了稀疏（Sparse）结构，比如GPT 3、PaLM、GLaM等，GPT 4大概率也会走稀疏模型路线。之所以采用Sparse 化的模型，主要好处是它可以极大减少LLM的训练时间和在线推理时间。Switch Transformer论文里指出：在相同算力预算的前提下，使用稀疏化Transformer，相对Dense Transformer，LLM模型的训练速度可以提升4倍到7倍。为何Sparse模型可以加快训练和推理时间呢？这是因为尽管模型参数巨大，但是对于某个训练实例，Sparse模型通过路由机制，只使用整个参数中的一小部分，参与训练和推理的活跃参数量比较少，所以速度快。</p><p>我认为未来超大的LLM模型大概率会收敛到稀疏模型。主要有两个原因：一方面，现有研究表明（参考：Large Models are Parsimonious Learners: Activation Sparsity in Trained Transformers），标准的Dense Transformer在训练和推理时，它本身也是稀疏激活的，就是说只有部分参数会被激活，大部分参数没有参与训练和推理过程。既然这样，我们不如直接迁移到稀疏模型；另外，毫无疑问LLM模型的规模会继续推大，而高昂的训练成本是妨碍其进一步扩大模型的重要阻力，使用稀疏模型可以极大降低超大模型的训练成本，所以随着模型规模越大，稀疏模型带来的收益越明显。考虑到这两个方面，大概率未来更大的LLM模型会采用稀疏模型方案。</p><p>那为何目前其它大规模模型不走稀疏模型的路线呢？因为Sparse模型存在训练不稳定、容易过拟合等问题，不太容易训练好。所以，如何修正稀疏模型面临的问题，设计出更容易训练的稀疏模型，是很重要的未来研究方向。</p><h2 id="取经之路：复刻ChatGPT时要注意些什么"><a href="#取经之路：复刻ChatGPT时要注意些什么" class="headerlink" title="取经之路：复刻ChatGPT时要注意些什么"></a>取经之路：复刻ChatGPT时要注意些什么</h2><p>如果希望能复刻类似ChatGPT这种效果令人惊艳的LLM模型，综合目前的各种研究结论，在做技术选型时需要重点权衡如下问题：</p><p>首先，在预训练模式上，我们有三种选择：GPT这种自回归语言模型，Bert这种双向语言模型，以及T5这种混合模式(Encoder-Decoder架构，在Encoder采取双向语言模型，Decoder采取自回归语言模型，所以是一种混合结构，但其本质仍属于Bert模式)。我们应选择GPT这种自回归语言模型，其原因在本文范式转换部分有做分析。目前看，国内LLM在做这方面技术选型的时候，貌似很多都走了Bert双向语言模型或T5混合语言模型的技术路线，很可能方向走偏了。</p><p>第二，强大的推理能力是让用户认可LLM的重要心理基础，而如果希望LLM能够具备强大的推理能力，根据目前经验，最好在做预训练的时候，要引入大量代码和文本一起进行LLM训练。至于其中的道理，在本文前面相关部分有对应分析。</p><p>第三，如果希望模型参数规模不要那么巨大，但又希望效果仍然足够好，此时有两个技术选项可做配置：要么增强高质量数据收集、挖掘、清理等方面的工作，意思是我模型参数可以是ChatGPT&#x2F;GPT 4的一半，但是要想达到类似的效果，那么高质量训练数据的数量就需要是ChatGPT&#x2F;GPT 4模型的一倍（Chinchilla的路子）；另外一个可以有效减小模型规模的路线是采取文本检索（Retrieval based）模型+LLM的路线，这样也可以在效果相当的前提下，极大减少LLM模型的参数规模。这两个技术选型不互斥，反而是互补的，也即是说，可以同时采取这两个技术，在模型规模相对比较小的前提下，达到超级大模型类似的效果。</p><p>第四，超级大模型因为模型规模大，所以训练成本过高，导致很少有机构有能力去做这件事。而且由上文分析可见，继续不断推大LLM模型规模是肯定会发生、也应该去做的事情。于是，如何通过技术手段降低LLM的训练成本就很重要。LLM的特征抽取器Sparse化是有效降低模型训练及推理成本的技术选择。由此可见，随着模型越来越大，LLM模型Sparse化是一个应该考虑的选项。</p><p>第五，ChatGPT是目前最接近理想LLM的技术方案，而理想中的LLM应该是以一个几乎无所不能的基础通用大模型作为依托，来支持各种各样的上层任务类型。目前看，支持越来越多的任务类型，主要是通过增加LLM预训练数据的多样性来达成的，数据多样性越好，LLM能够支持的任务类型就越丰富。所以，应该重视通过增加数据多样性来增加LLM新能力的思路。</p><p>第六，易用的人机操作接口。人类用他们自己习惯的表达方式来描述任务，而LLM要能够理解这些Instruct的真实含义。另外，也要注意这些Instruct是符合人类真实需求的，就是说，要从最终用户那里收集任务表述方式，而不能靠研发人员自己的臆想或猜测。ChatGPT给我最大的启发其实是这一点，至于是否用增强学习我倒觉得不重要，其它替代技术应该也能做类似的事情。</p><h2 id="ChatGPT-为什么是OpenAI"><a href="#ChatGPT-为什么是OpenAI" class="headerlink" title="ChatGPT:为什么是OpenAI"></a>ChatGPT:为什么是OpenAI</h2><p>为什么是OpenAI作出了ChatGPT，而不是其它机构呢？我们在这里可以做个简单分析。</p><p>在本文开头，我们提到了OpenAI看待LLM的理念。OpenAI是怎么看待LLM的呢？回顾它不断推出的技术，可以看出，它其实从GPT 1.0开始，基本就坚定地把LLM看做是通往AGI的一条必由之路。具体而言，在OpenAI眼中，未来的AGI应该长这个样子：有一个任务无关的超大型LLM，用来从海量数据中学习各种知识，这个LLM以生成一切的方式，来解决各种各样的实际问题，而且它应该能听懂人类的命令，以便于人类使用。其实对LLM发展理念的理解，在前半部分，就是“构建一个任务无关的超大型LLM，让它从海量数据中学习各种知识”，这一点几乎是大家的共识，能体现出OpenAI眼光的其实是后半部分。</p><p>OpenAI的理念比较超前，对自我定位从一开始就定得比较高，始终坚定不移地探索上述方式是否可以实现AGI。OpenAI之所以能作出ChatGPT，胜在一个是定位比较高，另一个是不受外界干扰，态度上坚定不移。</p><p>我们可以回顾下它走的一些关键路程：GPT 1.0走的是生成模式的自回归语言模型路线，比Bert出来的还早些。Bert证明了：双向语言模型对于很多NLP理解类任务，效果比自回归这种单向语言模型效果更好。尽管如此，GPT 2.0并没有因此切换到双向语言模型这条路上，仍然走文本生成的路，而且开始尝试零示例（zero shot）prompt和少量示例（few shot）prompt。其实这时候， OpenAI心目中的AGI已经开始浮出水面，逐渐显示出轮廓了。只是因为zero shot&#x2F;few shot效果比Bert+fine-tuning差的比较远，所以大家都没太当回事，甚至不理解它为什么要始终坚持走单向语言模型的路线。这个时候，我估计即使是OpenAI自己，也不一定能确保这条路肯定能走通。</p><p>但是，这不妨碍它继续在这条路上往后走。GPT 3.0已经展示出了比较强大的zero shot&#x2F;few shot prompt能力，这时候OpenAI心目中的AGI已经完全漏出水面，轮廓清晰，而且它的效果也证明了这条路，是有较大可能走得通的。GPT 3.0是一个决定LLM发展方向的叉路口和分水岭，与之对应的另外一条路是“Bert+fine-tuning”模式。在这个岔路口，不同的从业者选择走上了不同的道路，后面的技术差距也是从这里开始拉开的。很遗憾地是，国内很多从业者选择继续在“Bert+fine-tuning”这条路上往后走，这也是造成今天落后局面的一个关键时间节点。再往后，就是InstructGPT和ChatGPT了，OpenAI通过ChatGPT证明了一点；虽然我们距离真正的AGI，可能还有很长的路要走，但是通过超大LLM走向AGI这条路，目前看是可行的。</p>]]></content>
    
    
    
    <tags>
      
      <tag>技术介绍</tag>
      
      <tag>LLM</tag>
      
      <tag>大语言模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>北邮人工智能论坛 王小捷老师讲座--ChatGPT</title>
    <link href="/2023/06/06/2/"/>
    <url>/2023/06/06/2/</url>
    
    <content type="html"><![CDATA[<p>本讲座大纲：<br>ChatGPT概述 05:00<br>ChatGPT技术 11:30<br>问题与思考 41:30<br>结论 56:50</p><div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;"><iframe src="//player.bilibili.com/player.html?aid=695271908&bvid=BV1G24y187yx&cid=1033222609&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;"> </iframe></div>]]></content>
    
    
    
    <tags>
      
      <tag>ChatGPT</tag>
      
      <tag>概述</tag>
      
      <tag>技术介绍</tag>
      
      <tag>技术问题</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>教学案例设计文档</title>
    <link href="/2023/06/06/1/"/>
    <url>/2023/06/06/1/</url>
    
    <content type="html"><![CDATA[<p><strong>【基本信息】</strong></p><p>课程名称 ： 教育信息化发展研究 </p><p>课程创新作品名称：    《生成式人工智能的教育应用》                  </p><p>课程创新作品类型：        教学设计案例             </p><p><strong>【案例简介】</strong></p><p>《教育信息化发展研究》是面向教育技术学系三年级学生的一门专业基础课程，该课例《生成式人工智能的教育应用》为该课程第8讲“教育信息化发展趋势”的一部分，力图使学生了解生成式人工智能（以chatGPT为代表）的技术特点与应用潜力，并使学生能够辨析相关人工智能应用给新时代教育带来的机遇与风险。</p><p><strong>【教学团队】</strong></p><p>陈张乐（209025008）</p><p>王玉琦（209025009）</p><p>刘宇乐（209025015）</p><p>陆文慧（209025020）</p><p>陈紫玉（209025021）</p><p><strong>【适用对象】</strong></p><p>面向教育技术学专业本科三年级学生，以及软件工程专业、数字媒体专业等专业中对教育信息化发展趋势及未来教育感兴趣的学生。</p><p><strong>【所用教学媒体和资源】</strong></p><p>该课程为线上线下融合式课程，所以教学媒体包括：课堂交互课件、学生汇报课件、课堂演示实验、线上教育资源平台。（线上教育资源平台以专题网站的形式呈现）</p><p>资源主要涉及三方面：</p><p>1、生成式人工智能技术解读：OpenAI官方文档、chatGPT及New Bing试用体验、论文、科普视频等。</p><p>2、生成式人工智能的更广泛影响：学生观点、新闻报道、论文等。</p><p>3、教育界对生成式人工智能的看法：会议视频、论文、研讨会议文件等。</p><p><strong>【所涉知识点】</strong></p><p>1、生成式人工智能的技术原理</p><p>2、生成式人工智能的应用潜力</p><p>3、生成式人工智能助力教育信息化发展</p><p>4、生成式人工智能教育应用的机遇与风险</p><p><strong>【学情分析】</strong></p><p>该课面向教育技术学专业本科三年级学生，已经学习了《教学系统设计》、《学习科学与技术》、《教学资源设计与开发》等一系列教育技术学专业课，具备了较深的教育技术学理论与实践基础，知道如何设计教学资源，如何选择教学媒体，如何将信息技术与课程教学深度融合等。</p><p>此时，学生已学习了《教育信息化发展研究》前七讲，已经掌握了教育信息化的发展内涵、发展历程、相关政策法规、发展规划、发展现状等等内容，但是对教育信息化发展趋势不甚了解。</p><p>生成式人工智能作为一项新兴技术，同学对其技术原理的了解程度比较低，而且生成式人工智能的应用会带来知识产权、道德伦理等方面的诸多问题，同学未学习这方面的课程，因此还需进一步了解。</p><p><strong>【教学目标】</strong></p><p>1、通过阅读官方文档及相关文献，了解生成式人工智能的产生与发展历程 </p><p>2、了解生成式人工智能的运行模式与技术原理</p><p>3、熟悉生成式人工智能在各领域的应用潜力和经济影响</p><p>4、形成对生成式人工智能潜在教育应用以及其对教育系统产生的影响的认识，并能开展生成式人工智能与课堂教学融合的实践</p><p>5、通过资源研读及小组讨论，认识生成式人工智能教育应用的局限与风险，形成审慎的应用态度</p><p><strong>【教学内容与方法】</strong></p><p><strong>教学内容：</strong></p><p>第一课时：生成式人工智能的产生发展历程；生成式人工智能相关技术与核心能力；生成式人工智能的应用潜力与经济影响；</p><p>第二课时：生成式人工智能的潜在教育应用；生成式人工教育应用初步验证；生成式人工智能教育应用的局限与启示。</p><p><strong>教学方法：</strong></p><p>第一课时由于涉及相关技术，内容比较艰深，所以多用讲授法；</p><p>第二课时讲授生成式人工智能的教育应用，实践性、思辨性比较强，需要学生积极参与，所以主要使用演示法、讨论法、探究法，辅以讲授法。</p><p><strong>【教学重难点】</strong></p><p><strong>教学重点：</strong>让学生了解以chatGPT为代表的生成式人工智能背后的原理，以及所用到的技术，从而建立对生成式人工智能较完整的认知。</p><p><strong>教学难点：</strong>引导学生通过查阅文献、实际体验以及小组讨论，形成关于生成式人工智能的观点。</p><p><strong>【教学准备】</strong></p><p>1、明确教学流程：设计好教学环节，将该教学设计案例的内容分上下部分合理安排成两节课，第一节节课讲授生成式人工智能的缘起、原理、广泛应用；课后学生按照分配好的小组进行话题讨论，利用在线资源平台上的资源进行小组讨论并准备下节课的课堂汇报；第二节课进行小组汇报，教师负责引导、组织、总结。</p><p>2、准备好教学资源：精选与生成式人工智能相关的科普文章、科研文献、介绍视频、使用范例等等，将其分门别类上传到课程资源平台中，方便学生开展活动。</p><p>3、选用教学平台，进行学习监控：选用学习通平台作为该课程的辅助教学平台。教师可以通过学习通布置任务，控制作业完成进度，获取学生线上讨论记录。学生可以通过学习通来上传汇报PPT、抢答、小组互评等。</p><p><strong>【教学环节与内容】</strong></p><p><strong>第一课时：</strong></p><p><strong>一、课堂导入：</strong></p><p><strong>教师：</strong>播放GPT4官方介绍视频，提出问题：大家听说过chatGPT的鼎鼎大名吗？你知道GPT4是什么吗？</p><p><strong>学生：</strong>观看短片，带着问题探索新知。</p><p><strong>设计意图：</strong>以视频短片导入，通过可视化的方式向同学介绍GPT这类生成式人工智能，减轻认知负荷，实现快速导入。</p><p><strong>二、讲授新知：</strong></p><p><strong>（一）ChatGPT概述</strong></p><p><strong>教师：</strong>介绍ChatGPT自2022年11月30日发布以来收获的广泛关注。（展示用户注册数据、产业界的评论等）</p><p><strong>学生：</strong>了解ChatGPT的热度，思考其为何能吸引如此多用户。</p><p><strong>教师：</strong>进一步通过在线网站来演示ChatGPT的实际应用，并指导学生进行简单的实践。（<a href="https://chat.openai.com/%EF%BC%89">https://chat.openai.com/）</a></p><p><strong>学生：</strong>在教师指引下，探索ChatGPT的使用，感受ChatGPT的魅力。</p><p><strong>设计意图：</strong>初步介绍ChatGPT的缘由、使用方式，让学生感受其应用的魅力，通过简单的讲解及实验让学生建立起对ChatGPT的兴趣，思考其广泛应用。</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td></td><td><img src="/image/use.png" alt="chatGPT聊天界面"></td></tr></tbody></table><p><strong>实例：</strong></p><p><strong>教师：</strong>在归纳学生观点后，呈现chatGPT的潜在应用。</p><p>问答：开放领域问答，解答各种类型的问题；</p><p>对话：主要是聊天，并有一定的情感识别的能力；</p><p>写作：论文、诗歌、报告、教案、说明书、写小说等；</p><p>修改：各类文本的语言、结构以及情绪等各方面的修订；</p><p>摘要：各类文本自动生成各种要求下的摘要；</p><p>编码：写各种语言的程序代码。进行代码调试等</p><p>……</p><p><strong>教师：</strong>相较于以前的智能语言翻译程序、写诗程序等语言工具，大家觉得ChatGPT有哪些方面的提升？</p><p><strong>学生：</strong>边探索，边思考，甚至可以从ChatGPT寻求帮助，来回答问题。</p><p><strong>教师：</strong>ChatGPT相较于以前的语言工具，在几乎各方面都有显著的能力提升，如：</p><p>1、将大量之前认为是不同种类的语言应用任务统一在对话下，且在不同任务中都实现了较好的完成能力：</p><p>2、能够生成高度流畅的语言文本，且具有保持较长文本连续主题和逻辑结构的能力；</p><p>3、在对话过程中可以持续维护对用户信息的记忆，并能在多轮交互之后正确运用这些信息；</p><p>4、具有即时的答案交互修正能力，可以依据用户对当前答案的反馈信息即时修正答案的内容；</p><p>5、可能具有知道自己不知道某些事情的能力，并具有拒绝回答不知道的问题或一些特定问题的能力。</p><p><strong>（二）ChatGPT关键技术</strong></p><p><strong>教师：</strong>ChatGPT是当前自然语言处理领域的代表性成果。那么什么是自然语言处理呢？</p><p><strong>学生：</strong>思考何为自然语言处理。</p><p><strong>教师：</strong>介绍自然语言处理（NLP: Natural Language Processing）的定义：为自然语言现象建立计算模型，从而给语言学或心理语言现象提供计算解释，以及建立机器处理语言的技术方法，使机器能够完成语言处理任务。</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td></td><td><img src="/image/nlp.png" alt="自然语言处理发展历程"></td></tr></tbody></table><p><strong>教师：</strong>介绍自然语言处理的发展历程，chatGPT即属于大规模预训练语言模型的最新发展形态。随后介绍自然语言处理的关键技术：词向量表示，即把词表示为N维实数空间中的向量。这是一行字总基于某种预测算法在大规模语料上进行学习的技术，奠定了近年来NLP技术发展的基础。</p><p><strong>教师：</strong>随后提供一系列相关技术文献，要求同学利用10分钟时间选读文献，查找网络资料，了解chatGPT（Chat Generative Pre-training Transformer）这种预训练语言模型背后最关键的技术，并通过学习通进行抢答，回答“GPT如何从大规模无监督数据中学习知识并用于完成语言任务。”</p><p><strong>学生：</strong>利用课程资源平台上的资源进行自学，了解GPT的关键技术，回答问题。</p><p><strong>设计意图：</strong>在锻炼学生查找资料及阅读文献的能力同时，使学生了解GPT的关键技术。</p><p><strong>教师：</strong>在引导学生回答上述问题后，进一步拓展，讲解GPT技术的发展。</p><p>GPT技术主要包括四阶段：</p><p>第一阶段——GPT：从大规模无监督词向量学习转向大规模文本级别无监督学习，即无监督预训练；并利用特定任务的标注数据进行有监督的模型微调</p><p>第二阶段——GPT-2：研究人员认为当模型容量非常大且数据量足够丰富时，仅仅靠无监督学习获得的语言模型便可以完成其他语言任务，无需再进行适应。在技术上，预训练阶段保持不变，但是模型微调取消。同时模型参数规模、模型训练数据都变大，训练效果得到显著提高。</p><p>第三阶段——GPT-3：研究人员发现GPT-2在一些任务上做得还不够好，但是数据规模愈大，表现愈好。随着进一步实验，得出：模型性能强烈依赖于规模；更大的模型持续提升性能。</p><p>第四阶段——GPT-3.5：强大的AI系统行为不可预料，人类需要让AI 跟着人类意图走。因此研究人员提出了让语言模型对齐人类意图的三原则，即HHH三原则——Helpful, Honest and Harmless.基于此原则，在GPT-3基础上进行基于人类反馈的强化学习。</p><p><strong>学生：</strong>在听教师讲解chatGPT所用的核心技术时，形成对chatGPT这类生成式人工智能的理性认知，包括其成本、发展上限、应用范围及衍生的伦理问题等。</p><p><strong>三、课堂总结</strong></p><p><strong>教师：</strong>进行该节课的总结：chatGPT是人工智能领域理论方法、技术实施和工程落地应用的典范，是一个里程碑式的进展，影响范围设计社会经济文化各个领域；ChatGPT类应用带来了一系列技术与伦理挑战需要应对；对ChatGPT的研究不仅要针对ChatGPT本身及应用，还要追溯其源头；同时，人工智能研究不应局限在ChatGPT方法这一条路线上。</p><p><strong>学生：</strong>回顾该节课所讲，形成对ChatGPT类技术的整体认知，为下节讨论课的成功开展铺垫基础。</p><p><strong>第一课时课后：</strong></p><p><strong>教师通过学习通平台布置小组合作任务：</strong></p><p>仔细研读课程资源平台上的各类学习资源，开展小组合作，围绕“chatGPT教育应用”这一议题开展讨论，论述chatGPT类技术教育应用潜力、对教育行业产生的影响、对师生教与学产生的影响、如何规避或解决chatGPT教育应用所带来的学术诚信、知识产权、伦理道德等问题。每个小组需制作PPT，于第二课时进行5分钟内以内的课堂汇报。汇报需要体现小组讨论过程，不能直接照搬论文，观点须原创，表达同学真实的想法。</p><p><strong>学生以小组为单位合作完成任务：</strong></p><p>学生利用课程资源平台上载的各类资源进行自学，并以小组为单位开展组内讨论，形成对人工智能教育应用的看法。组内需要合理分工，撰写讲稿、制作PPT，确保讨论课的顺利进行。</p><p><strong>第二课时：</strong></p><p><strong>一、各小组汇报</strong></p><p><strong>教师：</strong>快速导入，组织各组按照要求依照次序进行5分钟以内的汇报，阐述小组观点。教师需要控制课堂进度，提醒小组汇报时长，维持课堂秩序，使得每位同学都参与进来，认真倾听其他同学的观点。每组汇报结束后，围绕该组观点组织学生进行点评和讨论，主要关注其汇报的五性（正确性：术语、概念、事实或观点是否存在错误；多样性：观点及分析解释是否全面；新颖性：是否关注技术前沿，观点及分析的创新性是否足够；完整性：观点及分析是否完整、清晰、连贯；易读性：PPT是否清晰易读）</p><p><strong>学生：</strong>以小组为单位，依照课前定好的顺序开展5分钟时长的小组观点汇报，论述小组对人工智能教育应用的看法。每组汇报结束后，其他同学应该提出自己的疑问与建议，进行简单的辩论，形成良好的思辨氛围。</p><p><strong>二、总结</strong></p><p>在所有小组汇报完成后，教师需再次强调chatGPT类技术的实质，其所运用到的关键技术，其教育应用的潜能，其教育应用可能存在的风险，呼吁同学在理解chatGPT类技术的内涵，把握教育本质的基础上开展教育应用。</p><p><strong>【教学反思】</strong></p><p>互联网内容生产经历了三种形态，分别是Web1.0的专业生成内容（PGC）、Web2.0的用户生成内容（UGC）以及Web3.0的人工智能生成内容（AIGC），以chatGPT为代表的生成式人工智能是Web3.0的关键技术支撑，可应用于区块链、元宇宙、全息世界，在未来教育中具有巨大的应用潜能。因此，我们认为需要把chatGPT类技术纳入《教育信息化发展研究》这门课程中。</p><p>本课的授课对象是教育技术学专业本课三年级学生，从他们已修课程类型来看，技术类课程涉及较少，尤其是新型信息技术，如人工智能、大数据等。而这些技术正在逐渐与课堂教学融合，已经产生了很多的应用实例，被多数地区所推广，典型如人工智能教育示范区。所以，教育技术学专业学生应该掌握这方面内容，一来是为教学做准备，而来是为组织学校教育信息化发展做准备。</p><p>正如前面提及的，授课对象这方面基础较为薄弱，没有修过人工智能、算法之类的技术先导课程，所以讲解还略有难度。第一节于是以讲授为主，学生讨论为辅，力求使学生形成对chatGPT类技术的基本认识。第一节课需要努力调动学生的学习兴趣，避免枯燥讲授，努力活跃课堂氛围，可以利用多种媒体激活学生注意。在引导学生亲身体验chatGPT魅力时，不仅需要解决好相关网络问题，还需要同时注意引导学生合理使用chatGPT，不能把chatGPT当作娱乐工具。</p><p>第二节课在第一节课的理论铺垫后，可以围绕学生对chatGPT教育应用的观点开展课堂教学。教师需要活跃讨论氛围，不能把讨论课上成汇报课，需要激发台下听众的思考热情与提问热情，促进台上台下学生观点的碰撞。因为chatGPT出现时间还比较短，学术界还未形成广泛共识与应用对策，所以课堂可以略微开放，注重学生观点表达，但是教师不能任由学生自由发挥，需要积极维系课堂健康的思想导向。这是上好一堂讨论课的关键。</p><p><strong>【案例特色】</strong></p><p>该课采用线上线下融合教学模式，采用翻转教学理念：第一节课教师讲基础，第一节课后学生自学强化，第二节课教师组织学生讨论。该课强调以学生为中心，强调课堂中各方的对话、思辨。这样能让学生在初次接触“生成式人工智能”领域时也能成为课堂的参与者、构建者，而非被动的知识接受者。两节课之间的讨论任务有效地将课堂学习延伸到课堂以外，提升课堂效率的同时，也给予学生更多互动的机会，从而促进知识的建构，提升学生的素质与能力，切实体现了教学设计的高阶性、创新性和挑战度。</p><p><strong>【预期实施效果和成效】</strong></p><p>通过这两节课的学习，经历了内容讲授、自主学习、观点讨论三个环节，学生能较好地掌握chatGPT类生成式人工智能的概念、技术原理，能认识到chatGPT类生成式人工智能的教育应用潜力与相关风险，并在学习和生活中寻找应用场景，开展小规模应用，为其在教育信息化实践中审慎、合理运用chatGPT类生成式人工智能奠定基础。</p><p><strong>【课程资源】</strong></p><p>课程资源平台网址：<a href="https://elapsela.github.io/">https://elapsela.github.io/</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
